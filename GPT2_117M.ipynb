{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1609,
     "status": "ok",
     "timestamp": 1561729977760,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "vCEi4BpONC7g",
    "outputId": "c77a75eb-0d30-4122-ef09-c2ebdff438da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 28 13:52:58 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   69C    P8    16W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rtt6IUfZ2QOl"
   },
   "source": [
    "## One-time installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCxX5ZQY1Iih"
   },
   "source": [
    "#### NVIDIA Apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 150545,
     "status": "ok",
     "timestamp": 1561730136422,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "PWHCFgUfu2Tw",
    "outputId": "18ad1d90-8d32-4b8b-9192-a4b4a0f8d1e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'apex'...\n",
      "remote: Enumerating objects: 168, done.\u001b[K\n",
      "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
      "remote: Compressing objects: 100% (105/105), done.\u001b[K\n",
      "remote: Total 4804 (delta 88), reused 117 (delta 63), pack-reused 4636\u001b[K\n",
      "Receiving objects: 100% (4804/4804), 8.79 MiB | 14.04 MiB/s, done.\n",
      "Resolving deltas: 100% (3100/3100), done.\n",
      "/content/apex\n",
      "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:244: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
      "  cmdoptions.check_install_build_global(options)\n",
      "Created temporary directory: /tmp/pip-ephem-wheel-cache-_1lro11i\n",
      "Created temporary directory: /tmp/pip-req-tracker-p7phz0wz\n",
      "Created requirements tracker '/tmp/pip-req-tracker-p7phz0wz'\n",
      "Created temporary directory: /tmp/pip-install-xywfqx7d\n",
      "Processing /content/apex\n",
      "  Created temporary directory: /tmp/pip-req-build-0h6h0j5z\n",
      "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-p7phz0wz'\n",
      "    Running setup.py (path:/tmp/pip-req-build-0h6h0j5z/setup.py) egg_info for package from file:///content/apex\n",
      "    Running command python setup.py egg_info\n",
      "    torch.__version__  =  1.1.0\n",
      "    running egg_info\n",
      "    creating pip-egg-info/apex.egg-info\n",
      "    writing pip-egg-info/apex.egg-info/PKG-INFO\n",
      "    writing dependency_links to pip-egg-info/apex.egg-info/dependency_links.txt\n",
      "    writing top-level names to pip-egg-info/apex.egg-info/top_level.txt\n",
      "    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\n",
      "    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\n",
      "  Source in /tmp/pip-req-build-0h6h0j5z has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
      "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-p7phz0wz'\n",
      "Skipping bdist_wheel for apex, due to binaries being disabled for it.\n",
      "Installing collected packages: apex\n",
      "  Created temporary directory: /tmp/pip-record-7qryqwd7\n",
      "    Running command /usr/bin/python3 -u -c 'import setuptools, tokenize;__file__='\"'\"'/tmp/pip-req-build-0h6h0j5z/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-7qryqwd7/install-record.txt --single-version-externally-managed --compile\n",
      "    torch.__version__  =  1.1.0\n",
      "\n",
      "    Compiling cuda extensions with\n",
      "    nvcc: NVIDIA (R) Cuda compiler driver\n",
      "    Copyright (c) 2005-2018 NVIDIA Corporation\n",
      "    Built on Sat_Aug_25_21:08:01_CDT_2018\n",
      "    Cuda compilation tools, release 10.0, V10.0.130\n",
      "    from /usr/local/cuda/bin\n",
      "\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-x86_64-3.6\n",
      "    creating build/lib.linux-x86_64-3.6/apex\n",
      "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
      "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "    copying apex/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
      "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
      "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
      "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
      "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
      "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
      "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    running build_ext\n",
      "    building 'apex_C' extension\n",
      "    creating build/temp.linux-x86_64-3.6\n",
      "    creating build/temp.linux-x86_64-3.6/csrc\n",
      "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
      "    building 'amp_C' extension\n",
      "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
      "    building 'fused_adam_cuda' extension\n",
      "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/fused_adam_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/fused_adam_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so\n",
      "    building 'syncbn' extension\n",
      "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
      "    building 'fused_layer_norm_cuda' extension\n",
      "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
      "    running install_lib\n",
      "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
      "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
      "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
      "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
      "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
      "    copying build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
      "    running install_egg_info\n",
      "    running egg_info\n",
      "    creating apex.egg-info\n",
      "    writing apex.egg-info/PKG-INFO\n",
      "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
      "    writing top-level names to apex.egg-info/top_level.txt\n",
      "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
      "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
      "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
      "    running install_scripts\n",
      "    writing list of installed files to '/tmp/pip-record-7qryqwd7/install-record.txt'\n",
      "  Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
      "  Removing source in /tmp/pip-req-build-0h6h0j5z\n",
      "Successfully installed apex-0.1\n",
      "Cleaning up...\n",
      "Removed build tracker '/tmp/pip-req-tracker-p7phz0wz'\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/apex\n",
    "% cd apex\n",
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\n",
    "% cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lj-TDb-P2UfL"
   },
   "source": [
    "#### BERT pretrained model repo (contains module for OpenAI GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 162082,
     "status": "ok",
     "timestamp": 1561730151730,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "nebCBnhhA0nP",
    "outputId": "241d2e00-c9e2-4ba5-bcdc-44e83e039d15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-pretrained-BERT'...\n",
      "remote: Enumerating objects: 3993, done.\u001b[K\n",
      "Receiving objects:   0% (1/3993)   \r",
      "Receiving objects:   1% (40/3993)   \r",
      "Receiving objects:   2% (80/3993)   \r",
      "Receiving objects:   3% (120/3993)   \r",
      "Receiving objects:   4% (160/3993)   \r",
      "Receiving objects:   5% (200/3993)   \r",
      "Receiving objects:   6% (240/3993)   \r",
      "Receiving objects:   7% (280/3993)   \r",
      "Receiving objects:   8% (320/3993)   \r",
      "Receiving objects:   9% (360/3993)   \r",
      "Receiving objects:  10% (400/3993)   \r",
      "Receiving objects:  11% (440/3993)   \r",
      "Receiving objects:  12% (480/3993)   \r",
      "Receiving objects:  13% (520/3993)   \r",
      "Receiving objects:  14% (560/3993)   \r",
      "Receiving objects:  15% (599/3993)   \r",
      "Receiving objects:  16% (639/3993)   \r",
      "Receiving objects:  17% (679/3993)   \r",
      "Receiving objects:  18% (719/3993)   \r",
      "Receiving objects:  19% (759/3993)   \r",
      "Receiving objects:  20% (799/3993)   \r",
      "Receiving objects:  21% (839/3993)   \r",
      "Receiving objects:  22% (879/3993)   \r",
      "Receiving objects:  23% (919/3993)   \r",
      "Receiving objects:  24% (959/3993)   \r",
      "Receiving objects:  25% (999/3993)   \r",
      "Receiving objects:  26% (1039/3993)   \r",
      "Receiving objects:  27% (1079/3993)   \r",
      "Receiving objects:  28% (1119/3993)   \r",
      "Receiving objects:  29% (1158/3993)   \r",
      "Receiving objects:  30% (1198/3993)   \r",
      "Receiving objects:  31% (1238/3993)   \r",
      "Receiving objects:  32% (1278/3993)   \r",
      "Receiving objects:  33% (1318/3993)   \r",
      "Receiving objects:  34% (1358/3993)   \r",
      "Receiving objects:  35% (1398/3993)   \r",
      "Receiving objects:  36% (1438/3993)   \r",
      "Receiving objects:  37% (1478/3993)   \r",
      "Receiving objects:  38% (1518/3993)   \r",
      "Receiving objects:  39% (1558/3993)   \r",
      "Receiving objects:  40% (1598/3993)   \r",
      "Receiving objects:  41% (1638/3993)   \r",
      "Receiving objects:  42% (1678/3993)   \r",
      "Receiving objects:  43% (1717/3993)   \r",
      "Receiving objects:  44% (1757/3993)   \r",
      "Receiving objects:  45% (1797/3993)   \r",
      "Receiving objects:  46% (1837/3993)   \r",
      "Receiving objects:  47% (1877/3993)   \r",
      "Receiving objects:  48% (1917/3993)   \r",
      "Receiving objects:  49% (1957/3993)   \r",
      "Receiving objects:  50% (1997/3993)   \r",
      "Receiving objects:  51% (2037/3993)   \r",
      "Receiving objects:  52% (2077/3993)   \r",
      "Receiving objects:  53% (2117/3993)   \r",
      "Receiving objects:  54% (2157/3993)   \r",
      "Receiving objects:  55% (2197/3993)   \r",
      "Receiving objects:  56% (2237/3993)   \r",
      "Receiving objects:  57% (2277/3993)   \r",
      "Receiving objects:  58% (2316/3993)   \r",
      "Receiving objects:  59% (2356/3993)   \r",
      "Receiving objects:  60% (2396/3993)   \r",
      "Receiving objects:  61% (2436/3993)   \r",
      "Receiving objects:  62% (2476/3993)   \r",
      "Receiving objects:  63% (2516/3993)   \r",
      "Receiving objects:  64% (2556/3993)   \r",
      "Receiving objects:  65% (2596/3993)   \r",
      "Receiving objects:  66% (2636/3993)   \r",
      "Receiving objects:  67% (2676/3993)   \r",
      "Receiving objects:  68% (2716/3993)   \r",
      "Receiving objects:  69% (2756/3993)   \r",
      "Receiving objects:  70% (2796/3993)   \r",
      "Receiving objects:  71% (2836/3993)   \r",
      "Receiving objects:  72% (2875/3993)   \r",
      "Receiving objects:  73% (2915/3993)   \r",
      "Receiving objects:  74% (2955/3993)   \r",
      "Receiving objects:  75% (2995/3993)   \r",
      "Receiving objects:  76% (3035/3993)   \r",
      "Receiving objects:  77% (3075/3993)   \r",
      "Receiving objects:  78% (3115/3993)   \r",
      "Receiving objects:  79% (3155/3993)   \r",
      "Receiving objects:  80% (3195/3993)   \r",
      "Receiving objects:  81% (3235/3993)   \r",
      "Receiving objects:  82% (3275/3993)   \r",
      "Receiving objects:  83% (3315/3993)   \r",
      "Receiving objects:  84% (3355/3993)   \r",
      "Receiving objects:  85% (3395/3993)   \r",
      "Receiving objects:  86% (3434/3993)   \r",
      "Receiving objects:  87% (3474/3993)   \r",
      "Receiving objects:  88% (3514/3993)   \r",
      "Receiving objects:  89% (3554/3993)   \r",
      "Receiving objects:  90% (3594/3993)   \r",
      "Receiving objects:  91% (3634/3993)   \r",
      "Receiving objects:  92% (3674/3993)   \r",
      "Receiving objects:  93% (3714/3993)   \r",
      "Receiving objects:  94% (3754/3993)   \r",
      "remote: Total 3993 (delta 0), reused 0 (delta 0), pack-reused 3993\u001b[K\n",
      "Receiving objects: 100% (3993/3993), 2.12 MiB | 19.05 MiB/s, done.\n",
      "Resolving deltas: 100% (2774/2774), done.\n",
      "Collecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
      "\u001b[K     |████████████████████████████████| 655kB 2.8MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: regex\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
      "Successfully built regex\n",
      "Installing collected packages: regex\n",
      "Successfully installed regex-2019.6.8\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/epsdg/pytorch-pretrained-BERT\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'pytorch-pretrained-BERT')\n",
    "\n",
    "! pip install regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYuMeLQ52Aak"
   },
   "source": [
    "#### Pretrained model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 188015,
     "status": "ok",
     "timestamp": 1561730182392,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "iw2LYEUD5HnP",
    "outputId": "1dbda5dc-f2e9-49ee-c90c-5f7383cd08a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\n",
      "Fetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\n",
      "Fetching hparams.json:   0%|                                            | 0.00/90.0 [00:00<?, ?it/s]\n",
      "Fetching model.ckpt.data-00000-of-00001:   0%|                          | 0.00/498M [00:00<?, ?it/s]\n",
      "Fetching model.ckpt.index:   0%|                                       | 0.00/5.21k [00:00<?, ?it/s]\n",
      "Fetching model.ckpt.meta:   0%|                                         | 0.00/471k [00:00<?, ?it/s]\n",
      "Fetching vocab.bpe:   0%|                                               | 0.00/456k [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   176  100   176    0     0   1795      0 --:--:-- --:--:-- --:--:--  1814\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  522M  100  522M    0     0  52.4M      0  0:00:09  0:00:09 --:--:-- 47.9M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1017k  100 1017k    0     0  5952k      0 --:--:-- --:--:-- --:--:-- 5952k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  445k  100  445k    0     0  2733k      0 --:--:-- --:--:-- --:--:-- 2733k\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_SIZE = '117M' # small\n",
    "\n",
    "subdir = os.path.join('models', MODEL_SIZE)\n",
    "if not os.path.exists(subdir):\n",
    "    os.makedirs(subdir)\n",
    "\n",
    "for filename in ['checkpoint','encoder.json','hparams.json','model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n",
    "\n",
    "    r = requests.get(\"https://storage.googleapis.com/gpt-2/\" + subdir + \"/\" + filename, stream=True)\n",
    "\n",
    "    with open(os.path.join(subdir, filename), 'wb') as f:\n",
    "        file_size = int(r.headers[\"content-length\"])\n",
    "        chunk_size = 1000\n",
    "        with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                f.write(chunk)\n",
    "pbar.update(chunk_size)\n",
    "\n",
    "! curl https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json --output '/content/models/117M/config.json'\n",
    "! curl https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin --output '/content/models/117M/pytorch_model.bin'\n",
    "\n",
    "! curl https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json --output '/content/models/117M/vocab.json'\n",
    "! curl https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt --output '/content/models/117M/merges.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zH6ulVK2uyI"
   },
   "source": [
    "## 2. Main routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1573,
     "status": "ok",
     "timestamp": 1561730205862,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "4r6hLWYkFBwL",
    "outputId": "cce86dc8-7b9a-4e96-fdea-8b99a1e62f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_cpus=2\n",
      "device=cuda, type: Tesla T4, CUDA capability: (7, 5)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from apex import amp\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'pytorch-pretrained-BERT')\n",
    "\n",
    "from pytorch_pretrained_bert import convert_gpt2_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import GPT2Tokenizer\n",
    "from pytorch_pretrained_bert import GPT2DoubleHeadsModel\n",
    "from pytorch_pretrained_bert import OpenAIAdam\n",
    "from pytorch_pretrained_bert import GPT2Config\n",
    "\n",
    "print(f'n_cpus={multiprocessing.cpu_count()}')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device={device}, '+\n",
    "      f'type: {torch.cuda.get_device_name(device)}, ' +\n",
    "      f'CUDA capability: {torch.cuda.get_device_capability(device)}')\n",
    "\n",
    "log_date = datetime.now().strftime('%m%d-%H%M')\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s: %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    filename='/content/GPT2-117M-' + log_date + '.txt',\n",
    "                    filemode='w')\n",
    "\n",
    "logger1 = logging.getLogger('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1EMBkrHxXb_"
   },
   "source": [
    "#### Convert model to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CdVaqfUwkbZe"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The config.json for the 117M version of GPT2 is missing several essential hparams.\n",
    "Manually add these to config.json:\n",
    "  \"resid_pdrop\": 0.1,\n",
    "  \"embd_pdrop\": 0.1,\n",
    "  \"attn_pdrop\": 0.1,\n",
    "  \"n_special\": 0,\n",
    "'''\n",
    "! mv /content/config.json /content/models/117M/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6983,
     "status": "ok",
     "timestamp": 1561730403905,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "9pvNNUnrG0MG",
    "outputId": "bd67264a-1001-4ec0-c1da-385e385c5e2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting TensorFlow checkpoint from /content/models/117M\n",
      "Loading TF weight model/h0/attn/c_attn/b with shape [2304]\n",
      "Loading TF weight model/h0/attn/c_attn/w with shape [1, 768, 2304]\n",
      "Loading TF weight model/h0/attn/c_proj/b with shape [768]\n",
      "Loading TF weight model/h0/attn/c_proj/w with shape [1, 768, 768]\n",
      "Loading TF weight model/h0/ln_1/b with shape [768]\n",
      "Loading TF weight model/h0/ln_1/g with shape [768]\n",
      "Loading TF weight model/h0/ln_2/b with shape [768]\n",
      "Loading TF weight model/h0/ln_2/g with shape [768]\n",
      "Loading TF weight model/h0/mlp/c_fc/b with shape [3072]\n",
      "Loading TF weight model/h0/mlp/c_fc/w with shape [1, 768, 3072]\n",
      "Loading TF weight model/h0/mlp/c_proj/b with shape [768]\n",
      "Loading TF weight model/h0/mlp/c_proj/w with shape [1, 3072, 768]\n",
      "Loading TF weight model/h1/attn/c_attn/b with shape [2304]\n",
      "Loading TF weight model/h1/attn/c_attn/w with shape [1, 768, 2304]\n",
      "Loading TF weight model/h1/attn/c_proj/b with shape [768]\n",
      "Loading TF weight model/h1/attn/c_proj/w with shape [1, 768, 768]\n",
      "Loading TF weight model/h1/ln_1/b with shape [768]\n",
      "Loading TF weight model/h1/ln_1/g with shape [768]\n",
      "Loading TF weight model/h1/ln_2/b with shape [768]\n",
      "Loading TF weight model/h1/ln_2/g with shape [768]\n",
      "Loading TF weight model/h1/mlp/c_fc/b with shape [3072]\n",
      "Loading TF weight model/h1/mlp/c_fc/w with shape [1, 768, 3072]\n",
      "Loading TF weight model/h1/mlp/c_proj/b with shape [768]\n",
      "Loading TF weight model/h1/mlp/c_proj/w with shape [1, 3072, 768]\n",
      "Loading TF weight model/h10/attn/c_attn/b with shape [2304]\n",
      "Loading TF weight model/h10/attn/c_attn/w with shape [1, 768, 2304]\n",
      "Loading TF weight model/h10/attn/c_proj/b with shape [768]\n",
      "Loading TF weight model/h10/attn/c_proj/w with shape [1, 768, 768]\n",
      "Loading TF weight model/h10/ln_1/b with shape [768]\n",
      "Loading TF weight model/h10/ln_1/g with shape [768]\n",
      "Loading TF weight model/h10/ln_2/b with shape [768]\n",
      "Loading TF weight model/h10/ln_2/g with shape [768]\n",
      "Loading TF weight model/h10/mlp/c_fc/b with shape [3072]\n",
      "Loading TF weight model/h10/mlp/c_fc/w with shape [1, 768, 3072]\n",
      "Loading TF weight model/h10/mlp/c_proj/b with shape [768]\n",
      "Loading TF weight model/h10/mlp/c_proj/w with shape [1, 3072, 768]\n",
      "Loading TF weight model/h11/attn/c_attn/b with shape [2304]\n",
      "Loading TF weight model/h11/attn/c_attn/w with shape [1, 768, 2304]\n",
      "Loading TF weight model/h11/attn/c_proj/b with shape [768]\n",
      "Loading TF weight model/h11/attn/c_proj/w with shape [1, 768, 768]\n",
      "Loading TF weight model/h11/ln_1/b with shape [768]\n",
      "Loading TF weight model/h11/ln_1/g with shape [768]\n",
      "Loading TF weight model/h11/ln_2/b with shape [768]\n",
      "Loading TF weight model/h11/ln_2/g with shape [768]\n",
      "Loading TF weight model/h11/mlp/c_fc/b with shape [3072]\n",
      "Loading TF weight model/h11/mlp/c_fc/w with shape [1, 768, 3072]\n",
      "Loading TF weight model/h11/mlp/c_proj/b with shape [768]\n",
      "Loading TF weight model/h11/mlp/c_proj/w with shape [1, 3072, 768]\n",
      "Loading TF weight model/h2/attn/c_attn/b with shape [2304]\n",
      "Loading TF weight model/h2/attn/c_attn/w with shape [1, 768, 2304]\n",
      "Loading TF weight model/h2/attn/c_proj/b with shape [768]\n",
      "Loading TF weight model/h2/attn/c_proj/w with shape [1, 768, 768]\n",
      "Loading TF weight model/h2/ln_1/b with shape [768]\n",
      "Loading TF weight model/h2/ln_1/g with shape [768]\n",
      "Loading TF weight model/h2/ln_2/b with shape [768]\n",
      "Loading TF weight model/h2/ln_2/g with shape [768]\n",
      "Loading TF weight model/h2/mlp/c_fc/b with shape [3072]\n",
      "Loading TF weight model/h2/mlp/c_fc/w with shape [1, 768, 3072]\n",
      "Loading TF weight model/h2/mlp/c_proj/b with shape [768]\n",
      "Loading TF weight model/h2/mlp/c_proj/w with shape [1, 3072, 768]\n",
      "Loading TF weight model/h3/attn/c_attn/b with shape [2304]\n",
      "Loading TF weight model/h3/attn/c_attn/w with shape [1, 768, 2304]\n",
      "Loading TF weight model/h3/attn/c_proj/b with shape [768]\n",
      "Loading TF weight model/h3/attn/c_proj/w with shape [1, 768, 768]\n",
      "Loading TF weight model/h3/ln_1/b with shape [768]\n",
      "Loading TF weight model/h3/ln_1/g with shape [768]\n",
      "Loading TF weight model/h3/ln_2/b with shape [768]\n",
      "Loading TF weight model/h3/ln_2/g with shape [768]\n",
      "Loading TF weight model/h3/mlp/c_fc/b with shape [3072]\n",
      "Loading TF weight model/h3/mlp/c_fc/w with shape [1, 768, 3072]\n",
      "Loading TF weight model/h3/mlp/c_proj/b with shape [768]\n",
      "Loading TF weight model/h3/mlp/c_proj/w with shape [1, 3072, 768]\n",
      "Loading TF weight model/h4/attn/c_attn/b with shape [2304]\n",
      "Loading TF weight model/h4/attn/c_attn/w with shape [1, 768, 2304]\n",
      "Loading TF weight model/h4/attn/c_proj/b with shape [768]\n",
      "Loading TF weight model/h4/attn/c_proj/w with shape [1, 768, 768]\n",
      "Loading TF weight model/h4/ln_1/b with shape [768]\n",
      "Loading TF weight model/h4/ln_1/g with shape [768]\n",
      "Loading TF weight model/h4/ln_2/b with shape [768]\n",
      "Loading TF weight model/h4/ln_2/g with shape [768]\n",
      "Loading TF weight model/h4/mlp/c_fc/b with shape [3072]\n",
      "Loading TF weight model/h4/mlp/c_fc/w with shape [1, 768, 3072]\n",
      "Loading TF weight model/h4/mlp/c_proj/b with shape [768]\n",
      "Loading TF weight model/h4/mlp/c_proj/w with shape [1, 3072, 768]\n",
      "Loading TF weight model/h5/attn/c_attn/b with shape [2304]\n",
      "Loading TF weight model/h5/attn/c_attn/w with shape [1, 768, 2304]\n",
      "Loading TF weight model/h5/attn/c_proj/b with shape [768]\n",
      "Loading TF weight model/h5/attn/c_proj/w with shape [1, 768, 768]\n",
      "Loading TF weight model/h5/ln_1/b with shape [768]\n",
      "Loading TF weight model/h5/ln_1/g with shape [768]\n",
      "Loading TF weight model/h5/ln_2/b with shape [768]\n",
      "Loading TF weight model/h5/ln_2/g with shape [768]\n",
      "Loading TF weight model/h5/mlp/c_fc/b with shape [3072]\n",
      "Loading TF weight model/h5/mlp/c_fc/w with shape [1, 768, 3072]\n",
      "Loading TF weight model/h5/mlp/c_proj/b with shape [768]\n",
      "Loading TF weight model/h5/mlp/c_proj/w with shape [1, 3072, 768]\n",
      "Loading TF weight model/h6/attn/c_attn/b with shape [2304]\n",
      "Loading TF weight model/h6/attn/c_attn/w with shape [1, 768, 2304]\n",
      "Loading TF weight model/h6/attn/c_proj/b with shape [768]\n",
      "Loading TF weight model/h6/attn/c_proj/w with shape [1, 768, 768]\n",
      "Loading TF weight model/h6/ln_1/b with shape [768]\n",
      "Loading TF weight model/h6/ln_1/g with shape [768]\n",
      "Loading TF weight model/h6/ln_2/b with shape [768]\n",
      "Loading TF weight model/h6/ln_2/g with shape [768]\n",
      "Loading TF weight model/h6/mlp/c_fc/b with shape [3072]\n",
      "Loading TF weight model/h6/mlp/c_fc/w with shape [1, 768, 3072]\n",
      "Loading TF weight model/h6/mlp/c_proj/b with shape [768]\n",
      "Loading TF weight model/h6/mlp/c_proj/w with shape [1, 3072, 768]\n",
      "Loading TF weight model/h7/attn/c_attn/b with shape [2304]\n",
      "Loading TF weight model/h7/attn/c_attn/w with shape [1, 768, 2304]\n",
      "Loading TF weight model/h7/attn/c_proj/b with shape [768]\n",
      "Loading TF weight model/h7/attn/c_proj/w with shape [1, 768, 768]\n",
      "Loading TF weight model/h7/ln_1/b with shape [768]\n",
      "Loading TF weight model/h7/ln_1/g with shape [768]\n",
      "Loading TF weight model/h7/ln_2/b with shape [768]\n",
      "Loading TF weight model/h7/ln_2/g with shape [768]\n",
      "Loading TF weight model/h7/mlp/c_fc/b with shape [3072]\n",
      "Loading TF weight model/h7/mlp/c_fc/w with shape [1, 768, 3072]\n",
      "Loading TF weight model/h7/mlp/c_proj/b with shape [768]\n",
      "Loading TF weight model/h7/mlp/c_proj/w with shape [1, 3072, 768]\n",
      "Loading TF weight model/h8/attn/c_attn/b with shape [2304]\n",
      "Loading TF weight model/h8/attn/c_attn/w with shape [1, 768, 2304]\n",
      "Loading TF weight model/h8/attn/c_proj/b with shape [768]\n",
      "Loading TF weight model/h8/attn/c_proj/w with shape [1, 768, 768]\n",
      "Loading TF weight model/h8/ln_1/b with shape [768]\n",
      "Loading TF weight model/h8/ln_1/g with shape [768]\n",
      "Loading TF weight model/h8/ln_2/b with shape [768]\n",
      "Loading TF weight model/h8/ln_2/g with shape [768]\n",
      "Loading TF weight model/h8/mlp/c_fc/b with shape [3072]\n",
      "Loading TF weight model/h8/mlp/c_fc/w with shape [1, 768, 3072]\n",
      "Loading TF weight model/h8/mlp/c_proj/b with shape [768]\n",
      "Loading TF weight model/h8/mlp/c_proj/w with shape [1, 3072, 768]\n",
      "Loading TF weight model/h9/attn/c_attn/b with shape [2304]\n",
      "Loading TF weight model/h9/attn/c_attn/w with shape [1, 768, 2304]\n",
      "Loading TF weight model/h9/attn/c_proj/b with shape [768]\n",
      "Loading TF weight model/h9/attn/c_proj/w with shape [1, 768, 768]\n",
      "Loading TF weight model/h9/ln_1/b with shape [768]\n",
      "Loading TF weight model/h9/ln_1/g with shape [768]\n",
      "Loading TF weight model/h9/ln_2/b with shape [768]\n",
      "Loading TF weight model/h9/ln_2/g with shape [768]\n",
      "Loading TF weight model/h9/mlp/c_fc/b with shape [3072]\n",
      "Loading TF weight model/h9/mlp/c_fc/w with shape [1, 768, 3072]\n",
      "Loading TF weight model/h9/mlp/c_proj/b with shape [768]\n",
      "Loading TF weight model/h9/mlp/c_proj/w with shape [1, 3072, 768]\n",
      "Loading TF weight model/ln_f/b with shape [768]\n",
      "Loading TF weight model/ln_f/g with shape [768]\n",
      "Loading TF weight model/wpe with shape [1024, 768]\n",
      "Loading TF weight model/wte with shape [50257, 768]\n",
      "Initialize PyTorch weight ['h0', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h0', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h0', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h0', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h0', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h0', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h0', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h0', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h0', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h0', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h0', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h0', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h1', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h1', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h1', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h1', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h1', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h1', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h1', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h1', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h1', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h1', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h1', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h1', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h10', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h10', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h10', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h10', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h10', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h10', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h10', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h10', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h10', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h10', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h10', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h10', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h11', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h11', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h11', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h11', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h11', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h11', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h11', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h11', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h11', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h11', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h11', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h11', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h2', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h2', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h2', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h2', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h2', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h2', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h2', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h2', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h2', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h2', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h2', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h2', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h3', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h3', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h3', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h3', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h3', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h3', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h3', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h3', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h3', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h3', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h3', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h3', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h4', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h4', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h4', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h4', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h4', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h4', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h4', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h4', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h4', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h4', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h4', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h4', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h5', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h5', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h5', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h5', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h5', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h5', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h5', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h5', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h5', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h5', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h5', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h5', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h6', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h6', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h6', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h6', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h6', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h6', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h6', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h6', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h6', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h6', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h6', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h6', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h7', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h7', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h7', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h7', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h7', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h7', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h7', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h7', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h7', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h7', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h7', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h7', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h8', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h8', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h8', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h8', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h8', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h8', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h8', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h8', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h8', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h8', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h8', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h8', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h9', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h9', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h9', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h9', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h9', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h9', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h9', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h9', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h9', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h9', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h9', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h9', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['ln_f', 'b']\n",
      "Initialize PyTorch weight ['ln_f', 'g']\n",
      "Initialize PyTorch weight ['wpe']\n",
      "Initialize PyTorch weight ['wte']\n",
      "Save PyTorch model to /content/models/117M/pytorch_model.bin\n",
      "Save configuration file to /content/models/117M/config.json\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = '/content/models/117M'\n",
    "\n",
    "convert_gpt2_checkpoint_to_pytorch.convert_gpt2_checkpoint_to_pytorch(\n",
    "    MODEL_PATH,\n",
    "    MODEL_PATH + '/config.json',\n",
    "    MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsbnNsmD240i"
   },
   "source": [
    "#### Load & process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17037,
     "status": "ok",
     "timestamp": 1561731804081,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "WQn4UwKfW_B1",
    "outputId": "9446d928-813a-4ca2-b215-7b5e03b15725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-tox id 103445 non-tox 200000 tox 144334\n",
      "train records: 633690 val records: 158423\n"
     ]
    }
   ],
   "source": [
    "TRAIN_VAL_SPLIT = 0.8\n",
    "\n",
    "def get_inputs(df_in, train_val_split):\n",
    "    # Returns: train_texts, train_labels, val_texts, val_labels\n",
    "    #    ( _texts: np.array of str )\n",
    "    #    ( labels: np.array of np.int64 )\n",
    "    train_df = train1[:int(train1.shape[0]*train_val_split)]\n",
    "    val_df = train1[int(train1.shape[0]*train_val_split):]\n",
    "    \n",
    "    train_texts = train_df.comment_text.values\n",
    "    train_labels = train_df.target_int.values.astype('int')\n",
    "    val_texts = val_df.comment_text.values\n",
    "    val_labels = val_df.target_int.values.astype('int')\n",
    "    \n",
    "    return train_texts, train_labels, val_texts, val_labels\n",
    "\n",
    "train_texts, train_labels, val_texts, val_labels = get_inputs(train1, TRAIN_VAL_SPLIT)\n",
    "\n",
    "print(f'train records: {len(train_texts)} val records: {len(val_texts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGVaeSo_TFhQ"
   },
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79xVuzs-R_u2"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 256 # text sequences truncated to MAX_LEN-2 to accomodate special tokens\n",
    "BATCH_SIZE = 16\n",
    "N_EPOCHS = 1\n",
    "ETA = 4e-6\n",
    "OPTIMIZER_WARMUP = 0.05\n",
    "GRAD_ACCUM_STEPS = 4 # gradient accumulation: step optimizer every # steps\n",
    "AMP_OPT_LEVEL = 'O1' # https://nvidia.github.io/apex/amp.html#opt-levels\n",
    "LM_COEFF = 0.01  # weighting for language modeling loss vs. classifier loss\n",
    "LOG_INTERVAL = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZE6SyYMpLV6d"
   },
   "source": [
    "#### Encode train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 186779,
     "status": "ok",
     "timestamp": 1561732079590,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "hF_pr6c9Qw8T",
    "outputId": "f46b5b3c-baea-4cdc-c69b-cb203323d9fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633690/633690 [03:03<00:00, 3454.80it/s]\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_TOKENS = ['<BOS>', '<SEP>', '<CLS>']\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH,\n",
    "                                               special_tokens=SPECIAL_TOKENS)\n",
    "\n",
    "special_tokens_ids = list(tokenizer.convert_tokens_to_ids(token) for token in SPECIAL_TOKENS)\n",
    "BOS, SEP, CLS = special_tokens_ids\n",
    "\n",
    "def encode(sentences, labels, tokenizer, max_len, special_tokens):\n",
    "    assert len(sentences) == len(labels)\n",
    "    n_records = len(sentences)\n",
    "\n",
    "    input_ids = np.zeros((n_records, 1, max_len), dtype=np.int64)\n",
    "    mc_token_ids = np.zeros((n_records, 1), dtype=np.int64)\n",
    "    lm_labels = np.full((n_records, 1, max_len), fill_value=-1, dtype=np.int64)\n",
    "    mc_labels = np.zeros((n_records, 1), dtype=np.int64)\n",
    "\n",
    "    for i, (sentence, label) in enumerate(tqdm(zip(sentences, labels),\n",
    "                                               total=len(sentences),\n",
    "                                               mininterval=10)):\n",
    " \n",
    "        tokens = tokenizer.tokenize(sentence)[:max_len-2]\n",
    "\n",
    "        indexed_tokens = [BOS] + tokenizer.convert_tokens_to_ids(tokens) + [CLS]\n",
    "        input_ids[i, 0, :len(indexed_tokens)] = indexed_tokens\n",
    "        mc_token_ids[i, 0] = len(indexed_tokens) - 1\n",
    "        lm_labels[i, 0, :len(indexed_tokens)] = indexed_tokens\n",
    "\n",
    "        mc_labels[i, 0] = label\n",
    "        \n",
    "    all_inputs = (input_ids, mc_token_ids, lm_labels, mc_labels)\n",
    "\n",
    "    return tuple(torch.tensor(t) for t in all_inputs)\n",
    "\n",
    "train_seqs = encode(train_texts,\n",
    "                    train_labels,\n",
    "                    tokenizer,\n",
    "                    MAX_LEN,\n",
    "                    SPECIAL_TOKENS)\n",
    "\n",
    "train_ds = TensorDataset(*train_seqs)\n",
    "train_sampler = RandomSampler(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvFLcBRs3LHa"
   },
   "source": [
    "#### Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n24FobbPXLs_"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = GPT2DoubleHeadsModel.from_pretrained(MODEL_PATH,\n",
    "                                                  num_special_tokens=len(SPECIAL_TOKENS))\n",
    "model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WC9PCpoMddD"
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 910
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 26788350,
     "status": "ok",
     "timestamp": 1561759096073,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "M0oKv7HlMeGe",
    "outputId": "b4c3cf3c-67ec-46f7-9e11-ed87686ed6cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|          | 0/79212 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 7887/79212 [44:13<6:41:54,  2.96it/s, clf_loss=0.467, lm_loss=7.77, lr=1.89e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 11892/79212 [1:06:47<6:19:03,  2.96it/s, clf_loss=0.318, lm_loss=6.57, lr=2.9e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 13939/79212 [1:18:18<6:07:34,  2.96it/s, clf_loss=0.286, lm_loss=6.12, lr=3.4e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 16431/79212 [1:32:44<5:53:43,  2.96it/s, clf_loss=0.279, lm_loss=5.65, lr=3.99e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▎       | 18656/79212 [1:44:53<5:41:05,  2.96it/s, clf_loss=0.266, lm_loss=5.39, lr=3.97e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 21059/79212 [1:58:25<5:27:43,  2.96it/s, clf_loss=0.258, lm_loss=5.26, lr=3.93e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 25064/79212 [2:21:00<5:05:26,  2.95it/s, clf_loss=0.24, lm_loss=5.01, lr=3.88e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 25242/79212 [2:22:00<5:04:16,  2.96it/s, clf_loss=0.24, lm_loss=5.01, lr=3.88e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 28357/79212 [2:39:34<4:46:39,  2.96it/s, clf_loss=0.252, lm_loss=4.95, lr=3.84e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 31116/79212 [2:55:07<4:31:30,  2.95it/s, clf_loss=0.243, lm_loss=4.88, lr=3.8e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 33252/79212 [3:07:09<4:19:05,  2.96it/s, clf_loss=0.233, lm_loss=4.83, lr=3.77e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 36189/79212 [3:23:43<4:02:36,  2.96it/s, clf_loss=0.223, lm_loss=4.74, lr=3.73e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▊     | 38592/79212 [3:37:16<3:49:16,  2.95it/s, clf_loss=0.243, lm_loss=4.72, lr=3.7e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 41084/79212 [3:51:19<3:35:14,  2.95it/s, clf_loss=0.23, lm_loss=4.7, lr=3.67e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▍    | 43398/79212 [4:04:22<3:21:57,  2.96it/s, clf_loss=0.226, lm_loss=4.71, lr=3.64e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 47403/79212 [4:26:58<2:59:32,  2.95it/s, clf_loss=0.229, lm_loss=4.66, lr=3.59e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 48916/79212 [4:35:30<2:50:55,  2.95it/s, clf_loss=0.231, lm_loss=4.6, lr=3.57e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|██████▌   | 51942/79212 [4:52:34<2:33:49,  2.95it/s, clf_loss=0.236, lm_loss=4.58, lr=3.53e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|██████▉   | 55057/79212 [5:10:08<2:16:19,  2.95it/s, clf_loss=0.215, lm_loss=4.56, lr=3.48e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  74%|███████▎  | 58261/79212 [5:28:13<1:58:14,  2.95it/s, clf_loss=0.213, lm_loss=4.53, lr=3.44e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  77%|███████▋  | 61287/79212 [5:45:18<1:41:13,  2.95it/s, clf_loss=0.226, lm_loss=4.52, lr=3.4e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  81%|████████  | 64135/79212 [6:01:22<1:25:05,  2.95it/s, clf_loss=0.211, lm_loss=4.52, lr=3.36e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|████████▍ | 66805/79212 [6:16:26<1:10:02,  2.95it/s, clf_loss=0.219, lm_loss=4.5, lr=3.33e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|████████▊ | 69653/79212 [6:32:30<53:55,  2.95it/s, clf_loss=0.211, lm_loss=4.5, lr=3.29e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  91%|█████████▏| 72323/79212 [6:47:35<38:53,  2.95it/s, clf_loss=0.226, lm_loss=4.47, lr=3.26e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|█████████▋| 76328/79212 [7:10:11<16:17,  2.95it/s, clf_loss=0.214, lm_loss=4.46, lr=3.2e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 78375/79212 [7:21:45<04:43,  2.95it/s, clf_loss=0.213, lm_loss=4.45, lr=3.18e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  99%|█████████▉| 78642/79212 [7:23:15<03:13,  2.95it/s, clf_loss=0.206, lm_loss=4.46, lr=3.17e-6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 79212/79212 [7:26:28<00:00,  2.96it/s, clf_loss=0.213, lm_loss=4.43, lr=3.16e-6]\n"
     ]
    }
   ],
   "source": [
    "optimizer = OpenAIAdam(optimizer_grouped_parameters,\n",
    "                       lr=ETA,\n",
    "                       warmup=OPTIMIZER_WARMUP,\n",
    "                       t_total=N_EPOCHS * np.ceil(len(train_texts) / BATCH_SIZE))\n",
    "\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=AMP_OPT_LEVEL, verbosity=1)\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "lm_losses = []\n",
    "clf_losses = []\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "logger1.info(f'train hparams:\\n   train recs: {len(train_texts):,}\\n'\n",
    "             + f'   max_len: {MAX_LEN}\\n   n_epochs: {N_EPOCHS}\\n'\n",
    "             + f'   batch size: {BATCH_SIZE}\\n   eta: {ETA}\\n'\n",
    "             + f'   accumulation steps: {GRAD_ACCUM_STEPS}\\n'\n",
    "             + f'   lm loss coeff: {LM_COEFF}\\n   opt_level: {AMP_OPT_LEVEL}'\n",
    "           )\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loader = DataLoader(train_ds,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=BATCH_SIZE)\n",
    "    n_batches = len(train_loader)\n",
    "    optimizer.zero_grad()\n",
    "    tq = tqdm(train_loader, desc=\"Training\", mininterval=30, maxinterval=60)\n",
    "\n",
    "    for step, batch in enumerate(tq):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        input_ids, mc_token_ids, lm_labels, mc_labels = batch\n",
    "\n",
    "        lm_loss, clf_loss = model(input_ids, mc_token_ids,\n",
    "                                  lm_labels,\n",
    "                                  mc_labels=mc_labels)\n",
    "                \n",
    "        loss = (LM_COEFF * lm_loss.to(device) + clf_loss.to(device)).to(device)\n",
    "\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "\n",
    "        loss = loss.to(device)\n",
    "\n",
    "        if (step+1) % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batch_lm_loss = lm_loss.item()\n",
    "        batch_clf_loss = clf_loss.item()\n",
    "        lm_losses.append(batch_lm_loss)\n",
    "        clf_losses.append(batch_clf_loss)\n",
    "\n",
    "        if (step+1) % LOG_INTERVAL == 0:\n",
    "            clf_loss_mean = sum(clf_losses[(step+1-LOG_INTERVAL):]) / LOG_INTERVAL\n",
    "            lm_loss_mean  = sum(lm_losses[(step+1-LOG_INTERVAL):]) / LOG_INTERVAL\n",
    "            lr = optimizer.get_lr()[0]\n",
    "            logstr = (f'step {step+1} of {n_batches} clf_loss {clf_loss_mean:.4f} '\n",
    "                      + f'lm_loss {lm_loss_mean:.4f} lr {lr:.4E}')\n",
    "            logger1.info(logstr)\n",
    "            tq.set_postfix(lm_loss=lm_loss_mean, clf_loss=clf_loss_mean, lr=lr)\n",
    "\n",
    "logger1.info('train complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyYVu2I2PB4i"
   },
   "source": [
    "#### Save trained model\n",
    "(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oiwRR4ZbJSue"
   },
   "outputs": [],
   "source": [
    "SAVED_MODEL_FNAME = 'gpt2_117M_pytorch.bin'\n",
    "SAVED_MODEL_DIR = '/content/saved_models'\n",
    "if not os.path.exists(SAVED_MODEL_DIR):\n",
    "    os.makedirs(SAVED_MODEL_DIR)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(SAVED_MODEL_DIR, SAVED_MODEL_FNAME))\n",
    "logger1.info(f'Model saved as {SAVED_MODEL_DIR + \"/\" + SAVED_MODEL_FNAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_qVnDcmRACA"
   },
   "source": [
    "#### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6392615,
     "status": "ok",
     "timestamp": 1561527759470,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "3S3kINED65Cc",
    "outputId": "27365199-67bf-4d4a-af7e-2a56e6937ad6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAADSCAYAAACo2xNAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBBC5DX/P8j5vzVpKA9eAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 3))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plt.plot(clf_losses, 'o', color='b', ms=1, alpha=0.1)\n",
    "plt.title('classification loss', fontsize=12, loc='left')\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "plt.plot(lm_losses, 'o', color='b', ms=1, alpha=0.1)\n",
    "plt.title('lm loss', fontsize=12, loc='left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZZmclZy3kT1"
   },
   "source": [
    "#### Encode eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 43279,
     "status": "ok",
     "timestamp": 1561760253056,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "u_bVKpFrO8p_",
    "outputId": "af42e077-0379-4928-8a4f-fc39e4d92940"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158423/158423 [00:42<00:00, 3719.59it/s]\n"
     ]
    }
   ],
   "source": [
    "val_seqs = encode(val_texts,\n",
    "                  val_labels,\n",
    "                  tokenizer,\n",
    "                  MAX_LEN,\n",
    "                  SPECIAL_TOKENS)\n",
    "\n",
    "val_ds = TensorDataset(*val_seqs)\n",
    "val_sampler = SequentialSampler(val_ds)\n",
    "\n",
    "val_loader = DataLoader(val_ds,\n",
    "                        sampler=val_sampler,\n",
    "                        batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A769rWgrPTK_"
   },
   "source": [
    "#### Load trained model\n",
    "Skip this step if training and evaluating in the same runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DBuFKv_5PMRN"
   },
   "outputs": [],
   "source": [
    "SAVED_MODEL_FNAME = 'gpt2_117M_pytorch.bin'\n",
    "SAVED_MODEL_DIR = '/content/saved_models'\n",
    "model.load_state_dict(torch.load(os.path.join(SAVED_MODEL_DIR, SAVED_MODEL_FNAME)))\n",
    "logger1.info(f'Model loaded from {SAVED_MODEL_DIR + \"/\" + SAVED_MODEL_FNAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iBIWAk4G3hBn"
   },
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2249505,
     "status": "ok",
     "timestamp": 1561762464535,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "KiAqw0gNXOP3",
    "outputId": "ebac0c20-bce6-4644-ef2f-1730365d7f94"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19803/19803 [36:51<00:00,  8.95it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "val_logits = []\n",
    "val_preds = []\n",
    "\n",
    "tq = tqdm(enumerate(val_loader), total=len(val_loader), mininterval=10)\n",
    "for i, batch in tq:\n",
    "    input_ids, mc_token_ids, _, _ = batch\n",
    "    \n",
    "    _, mc_logits_batch, _ = model(input_ids.to(device),\n",
    "                                  mc_token_ids.to(device))\n",
    "    \n",
    "    preds_batch = torch.sigmoid(mc_logits_batch).to(device)\n",
    "    preds_batch = preds_batch.detach().cpu().squeeze().tolist()\n",
    "    \n",
    "    val_preds.extend(preds_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1052907,
     "status": "ok",
     "timestamp": 1561762465166,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "VWNwO8R0O7Qb",
    "outputId": "873ad6e0-85a5-4d90-f22f-cde1b225ebab"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFrtJREFUeJzt3X+s3fV93/HnK3ZISBqwCbcWs9nM\nFLcdYUoCV8RRpq6NG2PIFCMtRaB1dpGFp0K6tqu2Odsf3iCRiLaVBSmh9YKHHbUhlDXDakw9yyGK\nNs3El5BCDGXcEAj2AN9iY9aiJHX63h/n4/TE33t9j+3re3zt50M6Op/v+/v5fs/nwzX3db4/zrmp\nKiRJ6vemYQ9AknTmMRwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6pg/7AGcrIsv\nvriWLl067GFI0pzx2GOP/XlVjQzSd86Gw9KlSxkbGxv2MCRpzkjywqB9BzqtlOS3kuxN8u0kX0zy\n1iSXJXk0yXiSLyU5r/V9S1seb+uX9u3nE63+TJJr+uqrWm08yYbBpypJOh2mDYcki4F/DoxW1RXA\nPOBG4NPAXVX1LuAQsK5tsg441Op3tX4kubxt925gFfC5JPOSzAM+C1wLXA7c1PpKkoZk0AvS84Hz\nk8wH3ga8BHwIeLCt3wJc39qr2zJt/YokafX7q+oHVfVdYBy4uj3Gq+q5qvohcH/rK0kakmnDoar2\nA/8R+B69UDgMPAa8VlVHWrd9wOLWXgy82LY90vq/s79+zDZT1SVJQzLIaaWF9N7JXwb8LeDt9E4L\nzbok65OMJRmbmJgYxhAk6ZwwyGmlXwK+W1UTVfVXwB8BHwQWtNNMAEuA/a29H7gUoK2/EHi1v37M\nNlPVO6pqU1WNVtXoyMhAd2NJkk7CIOHwPWB5kre1awcrgKeAR4CPtT5rgYdae1tbpq3/avX+3Nw2\n4MZ2N9NlwDLgG8AeYFm7++k8ehett5361CRJJ2vazzlU1aNJHgS+CRwBHgc2AV8B7k/yyVa7t21y\nL/CFJOPAQXq/7KmqvUkeoBcsR4DbqupHAEk+DuygdyfU5qraO3NTlCSdqMzVvyE9OjpafghOkgaX\n5LGqGh2k75z9hPSpWLrhK5PWn7/zI7M8Ekk6M/nFe5KkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkd\nhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdUwbDkl+Nsm3+h6v\nJ/nNJBcl2Znk2fa8sPVPkruTjCd5IsmVffta2/o/m2RtX/2qJE+2be5uf6takjQk04ZDVT1TVe+t\nqvcCVwFvAF8GNgC7qmoZsKstA1wLLGuP9cA9AEkuAjYC7weuBjYeDZTW55a+7VbNyOwkSSflRE8r\nrQC+U1UvAKuBLa2+Bbi+tVcDW6tnN7AgySXANcDOqjpYVYeAncCqtu6CqtpdvT9ovbVvX5KkITjR\ncLgR+GJrL6qql1r7ZWBRay8GXuzbZl+rHa++b5K6JGlIBg6HJOcBHwX+8Nh17R1/zeC4phrD+iRj\nScYmJiZO98tJ0jnrRI4crgW+WVWvtOVX2ikh2vOBVt8PXNq33ZJWO159yST1jqraVFWjVTU6MjJy\nAkOXJJ2IEwmHm/ibU0oA24CjdxytBR7qq69pdy0tBw630087gJVJFrYL0SuBHW3d60mWt7uU1vTt\nS5I0BPMH6ZTk7cCHgX/WV74TeCDJOuAF4IZW3w5cB4zTu7PpZoCqOpjkDmBP63d7VR1s7VuB+4Dz\ngYfbQ5I0JAOFQ1X9JfDOY2qv0rt76di+Bdw2xX42A5snqY8BVwwyFknS6ecnpCVJHYaDJKnDcJAk\ndRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH\n4SBJ6jAcJEkdhoMkqWOgcEiyIMmDSf4sydNJPpDkoiQ7kzzbnhe2vklyd5LxJE8kubJvP2tb/2eT\nrO2rX5XkybbN3Uky81OVJA1q0COHzwB/UlU/B7wHeBrYAOyqqmXArrYMcC2wrD3WA/cAJLkI2Ai8\nH7ga2Hg0UFqfW/q2W3Vq05IknYppwyHJhcDPA/cCVNUPq+o1YDWwpXXbAlzf2quBrdWzG1iQ5BLg\nGmBnVR2sqkPATmBVW3dBVe2uqgK29u1LkjQEgxw5XAZMAP81yeNJPp/k7cCiqnqp9XkZWNTai4EX\n+7bf12rHq++bpC5JGpJBwmE+cCVwT1W9D/hL/uYUEgDtHX/N/PB+UpL1ScaSjE1MTJzul5Okc9Yg\n4bAP2FdVj7blB+mFxSvtlBDt+UBbvx+4tG/7Ja12vPqSSeodVbWpqkaranRkZGSAoUuSTsa04VBV\nLwMvJvnZVloBPAVsA47ecbQWeKi1twFr2l1Ly4HD7fTTDmBlkoXtQvRKYEdb93qS5e0upTV9+5Ik\nDcH8Afv9OvD7Sc4DngNuphcsDyRZB7wA3ND6bgeuA8aBN1pfqupgkjuAPa3f7VV1sLVvBe4Dzgce\nbg9J0pAMFA5V9S1gdJJVKybpW8BtU+xnM7B5kvoYcMUgY5EknX5+QlqS1GE4SJI6DAdJUofhIEnq\nMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7D\nQZLUYThIkjoGCockzyd5Msm3koy12kVJdiZ5tj0vbPUkuTvJeJInklzZt5+1rf+zSdb21a9q+x9v\n22amJypJGtyJHDn8YlW9t6qO/i3pDcCuqloG7GrLANcCy9pjPXAP9MIE2Ai8H7ga2Hg0UFqfW/q2\nW3XSM5IknbJTOa20GtjS2luA6/vqW6tnN7AgySXANcDOqjpYVYeAncCqtu6CqtpdVQVs7duXJGkI\nBg2HAv5HkseSrG+1RVX1Umu/DCxq7cXAi33b7mu149X3TVLvSLI+yViSsYmJiQGHLkk6UfMH7PcP\nqmp/kp8Gdib5s/6VVVVJauaH95OqahOwCWB0dPS0v54knasGOnKoqv3t+QDwZXrXDF5pp4Rozwda\n9/3ApX2bL2m149WXTFKXJA3JtOGQ5O1J3nG0DawEvg1sA47ecbQWeKi1twFr2l1Ly4HD7fTTDmBl\nkoXtQvRKYEdb93qS5e0upTV9+5IkDcEgp5UWAV9ud5fOB/6gqv4kyR7ggSTrgBeAG1r/7cB1wDjw\nBnAzQFUdTHIHsKf1u72qDrb2rcB9wPnAw+0hSRqSacOhqp4D3jNJ/VVgxST1Am6bYl+bgc2T1MeA\nKwYYryRpFvgJaUlSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwk\nSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVLHwOGQZF6Sx5P8cVu+LMmjScaTfCnJea3+lrY8\n3tYv7dvHJ1r9mSTX9NVXtdp4kg0zNz1J0sk4kSOH3wCe7lv+NHBXVb0LOASsa/V1wKFWv6v1I8nl\nwI3Au4FVwOda4MwDPgtcC1wO3NT6SpKGZKBwSLIE+Ajw+bYc4EPAg63LFuD61l7dlmnrV7T+q4H7\nq+oHVfVdYBy4uj3Gq+q5qvohcH/rK0kakkGPHP4z8K+Av27L7wReq6ojbXkfsLi1FwMvArT1h1v/\nH9eP2WaquiRpSKYNhyT/CDhQVY/NwnimG8v6JGNJxiYmJoY9HEk6aw1y5PBB4KNJnqd3yudDwGeA\nBUnmtz5LgP2tvR+4FKCtvxB4tb9+zDZT1TuqalNVjVbV6MjIyABDlySdjGnDoao+UVVLqmopvQvK\nX62qfwI8AnysdVsLPNTa29oybf1Xq6pa/cZ2N9NlwDLgG8AeYFm7++m89hrbZmR2kqSTMn/6LlP6\n18D9ST4JPA7c2+r3Al9IMg4cpPfLnqram+QB4CngCHBbVf0IIMnHgR3APGBzVe09hXFJkk7RCYVD\nVX0N+FprP0fvTqNj+3wf+OUptv8U8KlJ6tuB7ScyFknS6eMnpCVJHYaDJKnDcJAkdRgOkqQOw0GS\n1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkd\nhoMkqWPacEjy1iTfSPKnSfYm+fetflmSR5OMJ/lSkvNa/S1tebytX9q3r0+0+jNJrumrr2q18SQb\nZn6akqQTMciRww+AD1XVe4D3AquSLAc+DdxVVe8CDgHrWv91wKFWv6v1I8nlwI3Au4FVwOeSzEsy\nD/gscC1wOXBT6ytJGpJpw6F6/qItvrk9CvgQ8GCrbwGub+3VbZm2fkWStPr9VfWDqvouMA5c3R7j\nVfVcVf0QuL/1lSQNyUDXHNo7/G8BB4CdwHeA16rqSOuyD1jc2ouBFwHa+sPAO/vrx2wzVX2ycaxP\nMpZkbGJiYpChS5JOwkDhUFU/qqr3AkvovdP/udM6qqnHsamqRqtqdGRkZBhDkKRzwgndrVRVrwGP\nAB8AFiSZ31YtAfa39n7gUoC2/kLg1f76MdtMVZckDckgdyuNJFnQ2ucDHwaephcSH2vd1gIPtfa2\ntkxb/9Wqqla/sd3NdBmwDPgGsAdY1u5+Oo/eRettMzE5SdLJmT99Fy4BtrS7it4EPFBVf5zkKeD+\nJJ8EHgfubf3vBb6QZBw4SO+XPVW1N8kDwFPAEeC2qvoRQJKPAzuAecDmqto7YzOUJJ2wacOhqp4A\n3jdJ/Tl61x+OrX8f+OUp9vUp4FOT1LcD2wcYryRpFvgJaUlSh+EgSeowHCRJHYaDJKnDcJAkdRgO\nkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVLHIH9D\n+tIkjyR5KsneJL/R6hcl2Znk2fa8sNWT5O4k40meSHJl377Wtv7PJlnbV78qyZNtm7uT5HRMVpI0\nmEGOHI4Av11VlwPLgduSXA5sAHZV1TJgV1sGuBZY1h7rgXugFybARuD99P686MajgdL63NK33apT\nn5ok6WRNGw5V9VJVfbO1/x/wNLAYWA1sad22ANe39mpga/XsBhYkuQS4BthZVQer6hCwE1jV1l1Q\nVburqoCtffuSJA3BCV1zSLIUeB/wKLCoql5qq14GFrX2YuDFvs32tdrx6vsmqUuShmTgcEjyU8B/\nA36zql7vX9fe8dcMj22yMaxPMpZkbGJi4nS/nCSdswYKhyRvphcMv19Vf9TKr7RTQrTnA62+H7i0\nb/MlrXa8+pJJ6h1VtamqRqtqdGRkZJChS5JOwiB3KwW4F3i6qn6nb9U24OgdR2uBh/rqa9pdS8uB\nw+300w5gZZKF7UL0SmBHW/d6kuXttdb07UuSNATzB+jzQeCfAk8m+Var/RvgTuCBJOuAF4Ab2rrt\nwHXAOPAGcDNAVR1Mcgewp/W7vaoOtvatwH3A+cDD7SFJGpJpw6Gq/icw1ecOVkzSv4DbptjXZmDz\nJPUx4IrpxiJJmh1+QlqS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiS\nOgwHSVLHIF+8J0maZUs3fGXS+vN3fmRWXt8jB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqSOacMh\nyeYkB5J8u692UZKdSZ5tzwtbPUnuTjKe5IkkV/Zts7b1fzbJ2r76VUmebNvcnWSqP0kqSZolgxw5\n3AesOqa2AdhVVcuAXW0Z4FpgWXusB+6BXpgAG4H3A1cDG48GSutzS992x76WJGmWTRsOVfV14OAx\n5dXAltbeAlzfV99aPbuBBUkuAa4BdlbVwao6BOwEVrV1F1TV7qoqYGvfviRJQ3Ky1xwWVdVLrf0y\nsKi1FwMv9vXb12rHq++bpD6pJOuTjCUZm5iYOMmhS5Kmc8oXpNs7/pqBsQzyWpuqarSqRkdGRmbj\nJSXpnHSy4fBKOyVEez7Q6vuBS/v6LWm149WXTFKXJA3RyYbDNuDoHUdrgYf66mvaXUvLgcPt9NMO\nYGWShe1C9EpgR1v3epLl7S6lNX37kiQNybTfyprki8AvABcn2UfvrqM7gQeSrANeAG5o3bcD1wHj\nwBvAzQBVdTDJHcCe1u/2qjp6kftWendEnQ883B6SpCGaNhyq6qYpVq2YpG8Bt02xn83A5knqY8AV\n041DkjR7/IS0JKnDcJAkdRgOkqQOw0GS1OHfkO4z7L/ZKklnCo8cJEkdhoMkqcNwkCR1GA6SpA7D\nQZLUYThIkjq8lXUA3uIq6VxjOEjSEE315nPYPK0kSerwyOEUeLpJ0qDO1COEqRgOkjSD5loITMVw\nOA1O9B+HRxrS3HO2hMBUzphwSLIK+AwwD/h8Vd055CHNGk9PSSfnbP8FPUxnRDgkmQd8FvgwsA/Y\nk2RbVT013JEN19nwD3+mAu5kAnSm/vud6BzOhp+bdEaEA3A1MF5VzwEkuR9YDZzT4XA2ON2/KGfj\nF7G/7HUuOlNuZV0MvNi3vK/VJElDcKYcOQwkyXpgfVv8iyTPnOSuLgb+fGZGNWc457PfuTZfOAfn\nnE+f0pz/zqAdz5Rw2A9c2re8pNV+QlVtAjad6oslGauq0VPdz1zinM9+59p8wTmfTmfKaaU9wLIk\nlyU5D7gR2DbkMUnSOeuMOHKoqiNJPg7soHcr6+aq2jvkYUnSOeuMCAeAqtoObJ+llzvlU1NzkHM+\n+51r8wXnfNqkqmbjdSRJc8iZcs1BknQGOavDIcmqJM8kGU+yYZL1b0nypbb+0SRLZ3+UM2eA+f6L\nJE8leSLJriQD39Z2pppuzn39/nGSSjLn72wZZM5Jbmg/671J/mC2xzjTBvi3/beTPJLk8fbv+7ph\njHOmJNmc5ECSb0+xPknubv89nkhy5YwPoqrOyge9C9vfAf4ucB7wp8Dlx/S5Ffjd1r4R+NKwx32a\n5/uLwNta+9fm8nwHnXPr9w7g68BuYHTY456Fn/My4HFgYVv+6WGPexbmvAn4tda+HHh+2OM+xTn/\nPHAl8O0p1l8HPAwEWA48OtNjOJuPHH78lRxV9UPg6Fdy9FsNbGntB4EVSTKLY5xJ0863qh6pqjfa\n4m56nyeZywb5GQPcAXwa+P5sDu40GWTOtwCfrapDAFV1YJbHONMGmXMBF7T2hcD/ncXxzbiq+jpw\n8DhdVgNbq2c3sCDJJTM5hrM5HAb5So4f96mqI8Bh4J2zMrqZd6JfQbKO3juPuWzaObfD7Uur6mz5\ngqRBfs4/A/xMkv+VZHf7xuO5bJA5/zvgV5Lso3fX46/PztCG5rR/5dAZcyurZk+SXwFGgX847LGc\nTkneBPwO8KtDHspsm0/v1NIv0Ds6/HqSv19Vrw11VKfXTcB9VfWfknwA+EKSK6rqr4c9sLnqbD5y\nGOQrOX7cJ8l8eoejr87K6GbeQF9BkuSXgH8LfLSqfjBLYztdppvzO4ArgK8leZ7eudltc/yi9CA/\n533Atqr6q6r6LvB/6IXFXDXInNcBDwBU1f8G3krve5fOVgP9/34qzuZwGOQrObYBa1v7Y8BXq13t\nmYOmnW+S9wG/Ry8Y5vp5aJhmzlV1uKourqqlVbWU3nWWj1bV2HCGOyMG+Xf93+kdNZDkYnqnmZ6b\nzUHOsEHm/D1gBUCSv0cvHCZmdZSzaxuwpt21tBw4XFUvzeQLnLWnlWqKr+RIcjswVlXbgHvpHX6O\n07v4c+PwRnxqBpzvfwB+CvjDdt39e1X10aEN+hQNOOezyoBz3gGsTPIU8CPgX1bVXD0iHnTOvw38\nlyS/Re/i9K/O4Td6JPkivYC/uF1H2Qi8GaCqfpfedZXrgHHgDeDmGR/DHP7vJ0k6Tc7m00qSpJNk\nOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpI7/DyXnT0SWT0R5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(val_preds, bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQezbZyNxNR7"
   },
   "source": [
    "#### Eval metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1047718,
     "status": "ok",
     "timestamp": 1561762465169,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "889FP6h5RLHm",
    "outputId": "bd06f5b8-cd6b-4bb4-d88c-0f6d5d182653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC 0.9673, acc 0.7467\n"
     ]
    }
   ],
   "source": [
    "val_preds = np.array(val_preds)\n",
    "val_AUC = metrics.roc_auc_score(val_labels, val_preds)\n",
    "val_acc = metrics.accuracy_score(val_labels.astype('int'), val_preds.astype('int'))\n",
    "logger1.info(f'Eval set AUC = {val_AUC:.4f}')\n",
    "logger1.info(f'Eval set accuracy = {val_acc:.4f}')\n",
    "print(f'AUC {val_AUC:.4f}, acc {val_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHll7y8abt1H"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GPT2_117M_v7.ipynb",
   "provenance": [
    {
     "file_id": "1RsZjlqL5VFcjjVtOJ_N5BdVSlDYqB85J",
     "timestamp": 1560881687844
    },
    {
     "file_id": "1HarqixjSTvW_C6nDvg2M2umlCVOQXrga",
     "timestamp": 1560867560358
    },
    {
     "file_id": "1RGHO_lRwx46YeT5H14g4x0wiGt5oEwnQ",
     "timestamp": 1560826213011
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
