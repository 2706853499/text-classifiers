{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 67131,
     "status": "ok",
     "timestamp": 1562681959493,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "OfOKa_LZ6qL6",
    "outputId": "d9891202-337e-4458-f551-c99cff0acc66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bert'...\n",
      "remote: Enumerating objects: 329, done.\u001b[K\n",
      "remote: Total 329 (delta 0), reused 0 (delta 0), pack-reused 329\u001b[K\n",
      "Receiving objects: 100% (329/329), 275.96 KiB | 3.68 MiB/s, done.\n",
      "Resolving deltas: 100% (181/181), done.\n",
      "Collecting tf_sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/fe/363d78d29c556d0da642ffe285c2c7573b6a83239a9b0d08d83376c9fbac/tf_sentencepiece-0.1.82.1-py2.py3-none-manylinux1_x86_64.whl (2.8MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8MB 3.5MB/s \n",
      "\u001b[?25hInstalling collected packages: tf-sentencepiece\n",
      "Successfully installed tf-sentencepiece-0.1.82.1\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/95/7f357995d5eb1131aa2092096dca14a6fc1b1d2860bd99c22a612e1d1019/sentencepiece-0.1.82-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 3.1MB/s \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.82\n",
      "--2019-07-09 14:18:24--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.112.128, 2607:f8b0:4001:c18::80\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.112.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1247797031 (1.2G) [application/zip]\n",
      "Saving to: ‘uncased_L-24_H-1024_A-16.zip’\n",
      "\n",
      "uncased_L-24_H-1024 100%[===================>]   1.16G  63.1MB/s    in 20s     \n",
      "\n",
      "2019-07-09 14:18:45 (60.7 MB/s) - ‘uncased_L-24_H-1024_A-16.zip’ saved [1247797031/1247797031]\n",
      "\n",
      "Archive:  /content/uncased_L-24_H-1024_A-16.zip\n",
      "   creating: uncased_L-24_H-1024_A-16/\n",
      "  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.meta  \n",
      "  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: uncased_L-24_H-1024_A-16/vocab.txt  \n",
      "  inflating: uncased_L-24_H-1024_A-16/bert_model.ckpt.index  \n",
      "  inflating: uncased_L-24_H-1024_A-16/bert_config.json  \n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/google-research/bert\n",
    "\n",
    "! pip install tf_sentencepiece\n",
    "! pip install sentencepiece\n",
    "\n",
    "! wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
    "! unzip /content/uncased_L-24_H-1024_A-16.zip\n",
    "! rm /content/uncased_L-24_H-1024_A-16.zip\n",
    "\n",
    "#https://storage.googleapis.com/bert_models/2019_05_30/wwm_uncased_L-24_H-1024_A-16.zip\n",
    "#https://storage.googleapis.com/bert_models/2019_05_30/wwm_cased_L-24_H-1024_A-16.zip\n",
    "#https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "#https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
    "#https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
    "#https://storage.googleapis.com/bert_models/2018_10_18/cased_L-24_H-1024_A-16.zip\n",
    "#https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
    "#https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2084,
     "status": "ok",
     "timestamp": 1562697069547,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "xcBsIgHKVkuJ",
    "outputId": "7eacd8c4-2074-4f65-f6ca-c28aff3f6d02"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "sys.path.insert(0, '/content/bert')\n",
    "\n",
    "from run_classifier import FLAGS\n",
    "FLAGS([None])\n",
    "\n",
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "\n",
    "from run_classifier import InputExample, PaddingInputExample, create_model\n",
    "from run_classifier import file_based_input_fn_builder\n",
    "from run_classifier import file_based_convert_examples_to_features\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "def clear_checkpoint_cache():\n",
    "  '''remove checkpoint files from FLAGS.output_dir (does not remove tf_record files)'''\n",
    "  files = tf.gfile.ListDirectory(FLAGS.output_dir)\n",
    "  files = [f for f in files if not f.endswith('tf_record')]\n",
    "  for file in files:\n",
    "    tf.gfile.Remove(os.path.join(FLAGS.output_dir, file))\n",
    "    \n",
    "def get_lens():\n",
    "  '''retrieve #_records for train & eval datasets'''\n",
    "  with open('./dataset_lens.json') as file:\n",
    "    lens = json.load(file)\n",
    "    ret = lens['train'], lens['val']\n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nerRoYYs4J4h"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3480,
     "status": "ok",
     "timestamp": 1562697079791,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "mVkmVQ7p_JSQ",
    "outputId": "ef341342-8c15-4fc7-b53a-29fa30359056"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPU devices:\n",
      "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 4624636520845270748),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 5862292550072502558),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 8113030125432651549),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 14384874521711867188),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 1112809580184861644),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 3079818582861333513),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 14918201319546221024),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 11325530939983350382),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 9260931485257681598),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 17589247473036768979),\n",
      " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 9402764591310561304)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 18:31:19.954918 139773671593856 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "\n",
    "with tf.Session(tpu_address) as session:\n",
    "  print('TPU devices:')\n",
    "  pprint.pprint(session.list_devices())\n",
    "\n",
    "  with open('/content/adc.json', 'r') as f:\n",
    "    auth_info = json.load(f)\n",
    "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddH594PD_xHq"
   },
   "outputs": [],
   "source": [
    "MODEL_SPEC = 'uncased_L-24_H-1024_A-16'\n",
    "\n",
    "BUCKET_NAME = ___________\n",
    "\n",
    "gcs_path = 'gs://' + BUCKET_NAME\n",
    "pretrained_model_path = gcs_path + '/' + MODEL_SPEC\n",
    "\n",
    "TRAIN_TFREC_FILE_ = 'train.tf_record'\n",
    "VAL_TFREC_FILE_ = 'val.tf_record'\n",
    "TEST_TFREC_FILE_ = 'test.tf_record'\n",
    "\n",
    "LABEL_LIST = [\"0\", \"1\"]\n",
    "\n",
    "FLAGS.task_name = 'bert_test'\n",
    "FLAGS.use_tpu = True\n",
    "FLAGS.num_tpu_cores = 8\n",
    "FLAGS.tpu_name = tpu_address\n",
    "\n",
    "FLAGS.do_lower_case = True\n",
    "\n",
    "FLAGS.max_seq_length = 512\n",
    "\n",
    "FLAGS.learning_rate = 5e-5\n",
    "FLAGS.train_batch_size = 32\n",
    "FLAGS.num_train_epochs = 0.1\n",
    "FLAGS.warmup_proportion = 0.1\n",
    "\n",
    "FLAGS.eval_batch_size = 8\n",
    "FLAGS.predict_batch_size = 8\n",
    "\n",
    "FLAGS.iterations_per_loop = 1000         # iterations per loop, NOT total train steps\n",
    "FLAGS.save_checkpoints_steps = 2000      # >= iterations\n",
    "MAX_SAVED_CHECKPOINTS = 3\n",
    "EVAL_ALL_CKPT = False\n",
    "\n",
    "FLAGS.output_dir = gcs_path + '/output'  # tf_record files AND model checkpoints\n",
    "\n",
    "FLAGS.vocab_file = pretrained_model_path + '/vocab.txt'\n",
    "FLAGS.bert_config_file = pretrained_model_path + '/bert_config.json' # must be on GCS\n",
    "FLAGS.init_checkpoint = pretrained_model_path + '/bert_model.ckpt' # must be on GCS\n",
    "\n",
    "train_tf_rec_file = FLAGS.output_dir + '/' + TRAIN_TFREC_FILE_\n",
    "val_tf_rec_file = FLAGS.output_dir + '/' + VAL_TFREC_FILE_\n",
    "test_tf_rec_file = FLAGS.output_dir + '/' + TEST_TFREC_FILE_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1542,
     "status": "ok",
     "timestamp": 1562697123743,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "o7WgHpmFmSAC",
    "outputId": "47eb2344-1f9e-4840-e79a-d86ca326656c"
   },
   "outputs": [],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n",
    "  raise ValueError(\n",
    "      \"Cannot use sequence length %d because the BERT model \"\n",
    "      \"was only trained up to sequence length %d\" %\n",
    "      (FLAGS.max_seq_length, bert_config.max_position_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9PuU0QDHcPyi"
   },
   "outputs": [],
   "source": [
    "# ONE TIME PER MODEL SPEC: move pretrained model to GS\n",
    "tf.gfile.MkDir(pretrained_model_path)\n",
    "model_files = os.listdir('./' + MODEL_SPEC)\n",
    "for file in model_files:\n",
    "  os.system('gsutil cp ./' + MODEL_SPEC + '/' + file + ' ' + pretrained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 36618,
     "status": "ok",
     "timestamp": 1562683269944,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "qoT1wOos8ZCH",
    "outputId": "1526f433-e056-475c-b71f-48e63c72ff39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train records: 1325501 val records: 331376\n"
     ]
    }
   ],
   "source": [
    "# ONE TIME PER FEATURE SET: import data and gen features\n",
    "\n",
    "def load_data():\n",
    "  # data load fn\n",
    "  return train_df, val_df\n",
    "\n",
    "train_df, val_df = load_data()\n",
    "\n",
    "print(f'train records: {len(train_df)} val records: {len(val_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3445197,
     "status": "ok",
     "timestamp": 1562688266630,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "jK1CK9sxAC-d",
    "outputId": "ed38441f-5239-4230-a990-7c48345303a9"
   },
   "outputs": [],
   "source": [
    "# ONE TIME PER FEATURE SET: encode inputs & convert to tf_record\n",
    "#\n",
    "# In the original bert.run_classifier.py, inputs are converted to features, then \n",
    "# saved as tf_record files every time a train run is initiated.  If reusing the\n",
    "# features already cached to output_dir, skip this step.\n",
    "#\n",
    "# Length of each dataset is required for computing train/eval steps, so lengths\n",
    "# are persisted to local FS as 'dataset_lens.json'.\n",
    "\n",
    "tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,\n",
    "                                                FLAGS.init_checkpoint)\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "def create_tf_rec_inputs(df_in, file_out, type=None):\n",
    "  assert type in ['train', 'val', 'test']\n",
    "  examples = []\n",
    "  for (idx, row) in tqdm(df_in.iterrows(), total=len(df_in), mininterval=10):\n",
    "\n",
    "    if type in ['train', 'val']:\n",
    "      example = InputExample(idx, row.comment_text, label=row.label)\n",
    "    elif type == 'test':\n",
    "      example = InputExample(idx, row.comment_text)\n",
    "\n",
    "    examples.append(example)\n",
    "\n",
    "  if type == 'train':\n",
    "    np.random.shuffle(examples)\n",
    "  \n",
    "  if type == 'val':\n",
    "    # for tpu evaluation, examples must be padded to a multiple of batch size\n",
    "    while len(examples) % FLAGS.eval_batch_size != 0:\n",
    "      examples.append(PaddingInputExample())\n",
    "    assert len(examples) % FLAGS.eval_batch_size == 0\n",
    "        \n",
    "  file_based_convert_examples_to_features(examples,\n",
    "                                          LABEL_LIST,\n",
    "                                          FLAGS.max_seq_length,\n",
    "                                          tokenizer,\n",
    "                                          file_out)\n",
    "  \n",
    "  return len(examples)\n",
    "\n",
    "train_len = create_tf_rec_inputs(train_df, train_tf_rec_file, type='train')\n",
    "val_len = create_tf_rec_inputs(val_df, val_tf_rec_file, type='val')\n",
    "#create_tf_rec_inputs(test_df, test_tf_rec_file, type='test')\n",
    "\n",
    "lens = {'train': train_len,\n",
    "       'val': val_len}\n",
    "with open('./dataset_lens.json', 'w') as file:\n",
    "  json.dump(lens, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3800,
     "status": "ok",
     "timestamp": 1562697138110,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "i7nUjsXfAszx",
    "outputId": "113ad86c-a92c-4691-9952-c864e535f521"
   },
   "outputs": [],
   "source": [
    "! gsutil ls $pretrained_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EaPWBXMqr8is"
   },
   "outputs": [],
   "source": [
    "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu_address)\n",
    "is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "\n",
    "run_config = tf.contrib.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    model_dir=FLAGS.output_dir,\n",
    "    save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "    keep_checkpoint_max=MAX_SAVED_CHECKPOINTS,\n",
    "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "        iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "        num_shards=FLAGS.num_tpu_cores,\n",
    "        per_host_input_for_training=is_per_host))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9IFZjKQpLG59"
   },
   "outputs": [],
   "source": [
    "# from bert.run_classifier.py -- added AUC to metric_fn\n",
    "\n",
    "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    tf.logging.info(\"*** Features ***\")\n",
    "    for name in sorted(features.keys()):\n",
    "      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "    is_real_example = None\n",
    "    if \"is_real_example\" in features:\n",
    "      is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
    "    else:\n",
    "      is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
    "        bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "        num_labels, use_one_hot_embeddings)\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    initialized_variable_names = {}\n",
    "    scaffold_fn = None\n",
    "    if init_checkpoint:\n",
    "      (assignment_map, initialized_variable_names\n",
    "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "      if use_tpu:\n",
    "\n",
    "        def tpu_scaffold():\n",
    "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "          return tf.train.Scaffold()\n",
    "\n",
    "        scaffold_fn = tpu_scaffold\n",
    "      else:\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "    tf.logging.info(\"**** Trainable Variables ****\")\n",
    "    for var in tvars:\n",
    "      init_string = \"\"\n",
    "      if var.name in initialized_variable_names:\n",
    "        init_string = \", *INIT_FROM_CKPT*\"\n",
    "      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "                      init_string)\n",
    "\n",
    "    output_spec = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "      train_op = optimization.create_optimizer(\n",
    "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          train_op=train_op,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\n",
    "      def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
    "        predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "        accuracy = tf.metrics.accuracy(\n",
    "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
    "        auc = tf.metrics.auc(\n",
    "            labels=label_ids, predictions=predictions, weights=is_real_example)\n",
    "        loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"eval_auc\": auc,\n",
    "            \"eval_loss\": loss,\n",
    "        }\n",
    "\n",
    "      eval_metrics = (metric_fn,\n",
    "                      [per_example_loss, label_ids, logits, is_real_example])\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          eval_metrics=eval_metrics,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    else:\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          predictions={\"probabilities\": probabilities},\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1562697148605,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "2s03ti5rGNJy",
    "outputId": "c715be8e-c43a-4dab-bda5-f9d4cffe583f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len 1325501 val len 331376\n",
      "num_train_steps 4142\n"
     ]
    }
   ],
   "source": [
    "train_len, val_len = get_lens()\n",
    "print('train len', train_len, 'val len', val_len)\n",
    "\n",
    "num_train_steps = int(train_len / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "print('num_train_steps', num_train_steps)\n",
    "\n",
    "model_fn = model_fn_builder(\n",
    "    bert_config=bert_config,\n",
    "    num_labels=len(LABEL_LIST),\n",
    "    init_checkpoint=FLAGS.init_checkpoint,\n",
    "    learning_rate=FLAGS.learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    use_tpu=FLAGS.use_tpu,\n",
    "    use_one_hot_embeddings=FLAGS.use_tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1562697156296,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "D4_6PLA5ACx8",
    "outputId": "6d793ca7-86d9-4003-ee2b-5398edf3f285"
   },
   "outputs": [],
   "source": [
    "estimator = tf.contrib.tpu.TPUEstimator(\n",
    "    use_tpu=FLAGS.use_tpu,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=FLAGS.train_batch_size,\n",
    "    predict_batch_size=FLAGS.predict_batch_size,\n",
    "    eval_batch_size=FLAGS.eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 208,
     "status": "ok",
     "timestamp": 1562697164005,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "T-BpeLnPGNP5",
    "outputId": "bac09232-4ce8-4b37-d1fb-52e3927675a4"
   },
   "outputs": [],
   "source": [
    "train_input_fn = file_based_input_fn_builder(\n",
    "  input_file=train_tf_rec_file,\n",
    "  seq_length=FLAGS.max_seq_length,\n",
    "  is_training=True,\n",
    "  drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1748,
     "status": "ok",
     "timestamp": 1562697170860,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "rG0frmqcLjcW",
    "outputId": "b746d47a-7c58-47d3-ff4c-d791477a27ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.tf_record', 'val.tf_record']"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_checkpoint_cache()\n",
    "tf.gfile.ListDirectory(FLAGS.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2682440,
     "status": "ok",
     "timestamp": 1562699867196,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "euyXj4gRhRmn",
    "outputId": "8950b69c-d026-41e6-af0c-c83feba08123"
   },
   "outputs": [],
   "source": [
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vrF87I0GgPeQ"
   },
   "outputs": [],
   "source": [
    "eval_input_fn = file_based_input_fn_builder(\n",
    "    input_file=val_tf_rec_file,\n",
    "    seq_length=FLAGS.max_seq_length,\n",
    "    is_training=False,\n",
    "    drop_remainder=True if FLAGS.use_tpu else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 419,
     "status": "ok",
     "timestamp": 1562700281526,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "d3oxZuzkAPtD",
    "outputId": "b083ec0c-4365-4a5d-da78-27bf5c5aadde"
   },
   "outputs": [],
   "source": [
    "steps_and_files = []\n",
    "filenames = tf.gfile.ListDirectory(FLAGS.output_dir)\n",
    "\n",
    "for filename in filenames:\n",
    "  if filename.endswith(\".index\"):\n",
    "    ckpt_name = filename[:-6]\n",
    "    cur_filename = os.path.join(FLAGS.output_dir, ckpt_name)\n",
    "    global_step = int(cur_filename.split(\"-\")[-1])\n",
    "    tf.logging.info(\"Add {} to eval list.\".format(cur_filename))\n",
    "    steps_and_files.append([global_step, cur_filename])\n",
    "steps_and_files = sorted(steps_and_files, key=lambda x: x[0])\n",
    "\n",
    "if not EVAL_ALL_CKPT:\n",
    "  steps_and_files = steps_and_files[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1192373,
     "status": "ok",
     "timestamp": 1562701490754,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "syLeMORZBI6b",
    "outputId": "18e028b4-bf1a-45d5-98d2-85450f428929"
   },
   "outputs": [],
   "source": [
    "eval_results = []\n",
    "eval_steps = val_len // FLAGS.eval_batch_size\n",
    "print('eval steps', eval_steps)\n",
    "\n",
    "for global_step, filename in sorted(steps_and_files, key=lambda x: x[0]):\n",
    "  ret = estimator.evaluate(\n",
    "      input_fn=eval_input_fn,\n",
    "      steps=eval_steps,\n",
    "      checkpoint_path=filename)\n",
    "  \n",
    "  ret[\"step\"] = global_step\n",
    "  ret[\"path\"] = filename\n",
    "\n",
    "  eval_results.append(ret)\n",
    "\n",
    "  tf.logging.info(\"=\" * 80)\n",
    "  log_str = \"Eval result | \"\n",
    "  for key, val in sorted(ret.items(), key=lambda x: x[0]):\n",
    "    log_str += \"{} {} | \".format(key, val)\n",
    "  tf.logging.info(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1562701644808,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "b25ktRlMG8FU",
    "outputId": "2b1fefea-b986-436a-ca9d-39feb600dff9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'eval_accuracy': 0.94855994,\n",
       "  'eval_auc': 0.9138441,\n",
       "  'eval_loss': 0.23178084,\n",
       "  'global_step': 4142,\n",
       "  'loss': 0.2222023,\n",
       "  'path': 'gs://bert422/output/model.ckpt-4142',\n",
       "  'step': 4142}]"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y9u1q62hG4v8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT_tf_v1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
