{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vCEi4BpONC7g"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rtt6IUfZ2QOl"
   },
   "source": [
    "## One-time installs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCxX5ZQY1Iih"
   },
   "source": [
    "#### NVIDIA Apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 151480,
     "status": "ok",
     "timestamp": 1561472278225,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "PWHCFgUfu2Tw",
    "outputId": "ca7e753f-13b9-423c-c3d4-cbc1f8b20655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'apex'...\n",
      "remote: Enumerating objects: 152, done.\u001b[K\n",
      "remote: Counting objects: 100% (152/152), done.\u001b[K\n",
      "remote: Compressing objects: 100% (95/95), done.\u001b[K\n",
      "remote: Total 4788 (delta 76), reused 104 (delta 57), pack-reused 4636\u001b[K\n",
      "Receiving objects: 100% (4788/4788), 8.79 MiB | 16.42 MiB/s, done.\n",
      "Resolving deltas: 100% (3088/3088), done.\n",
      "/content/apex\n",
      "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:244: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
      "  cmdoptions.check_install_build_global(options)\n",
      "Created temporary directory: /tmp/pip-ephem-wheel-cache-gbhdg0wf\n",
      "Created temporary directory: /tmp/pip-req-tracker-h0xnepf7\n",
      "Created requirements tracker '/tmp/pip-req-tracker-h0xnepf7'\n",
      "Created temporary directory: /tmp/pip-install-af5hro52\n",
      "Processing /content/apex\n",
      "  Created temporary directory: /tmp/pip-req-build-y0sgka82\n",
      "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-h0xnepf7'\n",
      "    Running setup.py (path:/tmp/pip-req-build-y0sgka82/setup.py) egg_info for package from file:///content/apex\n",
      "    Running command python setup.py egg_info\n",
      "    torch.__version__  =  1.1.0\n",
      "    running egg_info\n",
      "    creating pip-egg-info/apex.egg-info\n",
      "    writing pip-egg-info/apex.egg-info/PKG-INFO\n",
      "    writing dependency_links to pip-egg-info/apex.egg-info/dependency_links.txt\n",
      "    writing top-level names to pip-egg-info/apex.egg-info/top_level.txt\n",
      "    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\n",
      "    writing manifest file 'pip-egg-info/apex.egg-info/SOURCES.txt'\n",
      "  Source in /tmp/pip-req-build-y0sgka82 has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
      "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-h0xnepf7'\n",
      "Skipping bdist_wheel for apex, due to binaries being disabled for it.\n",
      "Installing collected packages: apex\n",
      "  Created temporary directory: /tmp/pip-record-wr97iaml\n",
      "    Running command /usr/bin/python3 -u -c 'import setuptools, tokenize;__file__='\"'\"'/tmp/pip-req-build-y0sgka82/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-wr97iaml/install-record.txt --single-version-externally-managed --compile\n",
      "    torch.__version__  =  1.1.0\n",
      "\n",
      "    Compiling cuda extensions with\n",
      "    nvcc: NVIDIA (R) Cuda compiler driver\n",
      "    Copyright (c) 2005-2018 NVIDIA Corporation\n",
      "    Built on Sat_Aug_25_21:08:01_CDT_2018\n",
      "    Cuda compilation tools, release 10.0, V10.0.130\n",
      "    from /usr/local/cuda/bin\n",
      "\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-x86_64-3.6\n",
      "    creating build/lib.linux-x86_64-3.6/apex\n",
      "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
      "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
      "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
      "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
      "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
      "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
      "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
      "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
      "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
      "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
      "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
      "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "    copying apex/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
      "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
      "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
      "    running build_ext\n",
      "    building 'apex_C' extension\n",
      "    creating build/temp.linux-x86_64-3.6\n",
      "    creating build/temp.linux-x86_64-3.6/csrc\n",
      "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
      "    building 'amp_C' extension\n",
      "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
      "    building 'fused_adam_cuda' extension\n",
      "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/fused_adam_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/fused_adam_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -O3 --use_fast_math -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda.o build/temp.linux-x86_64-3.6/csrc/fused_adam_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so\n",
      "    building 'syncbn' extension\n",
      "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
      "    building 'fused_layer_norm_cuda' extension\n",
      "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
      "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
      "    running install_lib\n",
      "    copying build/lib.linux-x86_64-3.6/fused_adam_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
      "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
      "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
      "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
      "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
      "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
      "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
      "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
      "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
      "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
      "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
      "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
      "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
      "    running install_egg_info\n",
      "    running egg_info\n",
      "    creating apex.egg-info\n",
      "    writing apex.egg-info/PKG-INFO\n",
      "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
      "    writing top-level names to apex.egg-info/top_level.txt\n",
      "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
      "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
      "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
      "    running install_scripts\n",
      "    writing list of installed files to '/tmp/pip-record-wr97iaml/install-record.txt'\n",
      "  Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
      "  Removing source in /tmp/pip-req-build-y0sgka82\n",
      "Successfully installed apex-0.1\n",
      "Cleaning up...\n",
      "Removed build tracker '/tmp/pip-req-tracker-h0xnepf7'\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/apex\n",
    "% cd apex\n",
    "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\n",
    "% cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lj-TDb-P2UfL"
   },
   "source": [
    "#### BERT pretrained model repo (contains module for OpenAI GPT2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 163064,
     "status": "ok",
     "timestamp": 1561472295265,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "nebCBnhhA0nP",
    "outputId": "da3ca39d-2c68-4f75-b740-843ac7acc38d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pytorch-pretrained-BERT'...\n",
      "remote: Enumerating objects: 54, done.\u001b[K\n",
      "remote: Counting objects:   1% (1/54)   \u001b[K\r",
      "remote: Counting objects:   3% (2/54)   \u001b[K\r",
      "remote: Counting objects:   5% (3/54)   \u001b[K\r",
      "remote: Counting objects:   7% (4/54)   \u001b[K\r",
      "remote: Counting objects:   9% (5/54)   \u001b[K\r",
      "remote: Counting objects:  11% (6/54)   \u001b[K\r",
      "remote: Counting objects:  12% (7/54)   \u001b[K\r",
      "remote: Counting objects:  14% (8/54)   \u001b[K\r",
      "remote: Counting objects:  16% (9/54)   \u001b[K\r",
      "remote: Counting objects:  18% (10/54)   \u001b[K\r",
      "remote: Counting objects:  20% (11/54)   \u001b[K\r",
      "remote: Counting objects:  22% (12/54)   \u001b[K\r",
      "remote: Counting objects:  24% (13/54)   \u001b[K\r",
      "remote: Counting objects:  25% (14/54)   \u001b[K\r",
      "remote: Counting objects:  27% (15/54)   \u001b[K\r",
      "remote: Counting objects:  29% (16/54)   \u001b[K\r",
      "remote: Counting objects:  31% (17/54)   \u001b[K\r",
      "remote: Counting objects:  33% (18/54)   \u001b[K\r",
      "remote: Counting objects:  35% (19/54)   \u001b[K\r",
      "remote: Counting objects:  37% (20/54)   \u001b[K\r",
      "remote: Counting objects:  38% (21/54)   \u001b[K\r",
      "remote: Counting objects:  40% (22/54)   \u001b[K\r",
      "remote: Counting objects:  42% (23/54)   \u001b[K\r",
      "remote: Counting objects:  44% (24/54)   \u001b[K\r",
      "remote: Counting objects:  46% (25/54)   \u001b[K\r",
      "remote: Counting objects:  48% (26/54)   \u001b[K\r",
      "remote: Counting objects:  50% (27/54)   \u001b[K\r",
      "remote: Counting objects:  51% (28/54)   \u001b[K\r",
      "remote: Counting objects:  53% (29/54)   \u001b[K\r",
      "remote: Counting objects:  55% (30/54)   \u001b[K\r",
      "remote: Counting objects:  57% (31/54)   \u001b[K\r",
      "remote: Counting objects:  59% (32/54)   \u001b[K\r",
      "remote: Counting objects:  61% (33/54)   \u001b[K\r",
      "remote: Counting objects:  62% (34/54)   \u001b[K\r",
      "remote: Counting objects:  64% (35/54)   \u001b[K\r",
      "remote: Counting objects:  66% (36/54)   \u001b[K\r",
      "remote: Counting objects:  68% (37/54)   \u001b[K\r",
      "remote: Counting objects:  70% (38/54)   \u001b[K\r",
      "remote: Counting objects:  72% (39/54)   \u001b[K\r",
      "remote: Counting objects:  74% (40/54)   \u001b[K\r",
      "remote: Counting objects:  75% (41/54)   \u001b[K\r",
      "remote: Counting objects:  77% (42/54)   \u001b[K\r",
      "remote: Counting objects:  79% (43/54)   \u001b[K\r",
      "remote: Counting objects:  81% (44/54)   \u001b[K\r",
      "remote: Counting objects:  83% (45/54)   \u001b[K\r",
      "remote: Counting objects:  85% (46/54)   \u001b[K\r",
      "remote: Counting objects:  87% (47/54)   \u001b[K\r",
      "remote: Counting objects:  88% (48/54)   \u001b[K\r",
      "remote: Counting objects:  90% (49/54)   \u001b[K\r",
      "remote: Counting objects:  92% (50/54)   \u001b[K\r",
      "remote: Counting objects:  94% (51/54)   \u001b[K\r",
      "remote: Counting objects:  96% (52/54)   \u001b[K\r",
      "remote: Counting objects:  98% (53/54)   \u001b[K\r",
      "remote: Counting objects: 100% (54/54)   \u001b[K\r",
      "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
      "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
      "remote: Total 3993 (delta 28), reused 29 (delta 15), pack-reused 3939\n",
      "Receiving objects: 100% (3993/3993), 2.20 MiB | 12.43 MiB/s, done.\n",
      "Resolving deltas: 100% (2760/2760), done.\n",
      "Collecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/4e/1b178c38c9a1a184288f72065a65ca01f3154df43c6ad898624149b8b4e0/regex-2019.06.08.tar.gz (651kB)\n",
      "\u001b[K     |████████████████████████████████| 655kB 4.9MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: regex\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Stored in directory: /root/.cache/pip/wheels/35/e4/80/abf3b33ba89cf65cd262af8a22a5a999cc28fbfabea6b38473\n",
      "Successfully built regex\n",
      "Installing collected packages: regex\n",
      "Successfully installed regex-2019.6.8\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/epsdg/pytorch-pretrained-BERT\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'pytorch-pretrained-BERT')\n",
    "\n",
    "! pip install regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYuMeLQ52Aak"
   },
   "source": [
    "#### Pretrained model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 233621,
     "status": "ok",
     "timestamp": 1561472376280,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "iw2LYEUD5HnP",
    "outputId": "616e9338-6864-4d0b-850d-a4c2f39cecf7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint:   0%|                                              | 0.00/77.0 [00:00<?, ?it/s]\n",
      "Fetching encoder.json:   0%|                                           | 0.00/1.04M [00:00<?, ?it/s]\n",
      "Fetching hparams.json:   0%|                                            | 0.00/91.0 [00:00<?, ?it/s]\n",
      "Fetching model.ckpt.data-00000-of-00001:   0%|                         | 0.00/1.42G [00:00<?, ?it/s]\n",
      "Fetching model.ckpt.index:   0%|                                       | 0.00/10.4k [00:00<?, ?it/s]\n",
      "Fetching model.ckpt.meta:   0%|                                         | 0.00/927k [00:00<?, ?it/s]\n",
      "Fetching vocab.bpe:   0%|                                               | 0.00/456k [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   293  100   293    0     0    813      0 --:--:-- --:--:-- --:--:--   816\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1449M  100 1449M    0     0  33.5M      0  0:00:43  0:00:43 --:--:-- 41.8M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1017k  100 1017k    0     0  1549k      0 --:--:-- --:--:-- --:--:-- 1546k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  445k  100  445k    0     0   732k      0 --:--:-- --:--:-- --:--:--  732k\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_SIZE = '345M' # medium\n",
    "\n",
    "subdir = os.path.join('models', MODEL_SIZE)\n",
    "if not os.path.exists(subdir):\n",
    "    os.makedirs(subdir)\n",
    "subdir = subdir.replace('\\\\','/') # needed for Windows\n",
    "\n",
    "for filename in ['checkpoint','encoder.json','hparams.json','model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n",
    "\n",
    "    r = requests.get(\"https://storage.googleapis.com/gpt-2/\" + subdir + \"/\" + filename, stream=True)\n",
    "\n",
    "    with open(os.path.join(subdir, filename), 'wb') as f:\n",
    "        file_size = int(r.headers[\"content-length\"])\n",
    "        chunk_size = 1000\n",
    "        with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
    "            # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                f.write(chunk)\n",
    "pbar.update(chunk_size)\n",
    "\n",
    "! curl https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-config.json --output '/content/models/345M/config.json'\n",
    "! curl https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-pytorch_model.bin --output '/content/models/345M/pytorch_model.bin'\n",
    "\n",
    "! curl https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json --output '/content/models/345M/vocab.json'\n",
    "! curl https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-merges.txt --output '/content/models/345M/merges.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2zH6ulVK2uyI"
   },
   "source": [
    "## 2. Main routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1561479978071,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "4r6hLWYkFBwL",
    "outputId": "a238a862-692c-47a5-f2da-bc54d44e85b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_cpus=2\n",
      "device=cuda, type: Tesla T4, CUDA capability: (7, 5)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import multiprocessing\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from apex import amp\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'pytorch-pretrained-BERT')\n",
    "\n",
    "from pytorch_pretrained_bert import convert_gpt2_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import GPT2Tokenizer\n",
    "from pytorch_pretrained_bert import GPT2DoubleHeadsModel\n",
    "from pytorch_pretrained_bert import OpenAIAdam\n",
    "from pytorch_pretrained_bert import GPT2Config\n",
    "\n",
    "print(f'n_cpus={multiprocessing.cpu_count()}')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'device={device}, '+\n",
    "      f'type: {torch.cuda.get_device_name(device)}, ' +\n",
    "      f'CUDA capability: {torch.cuda.get_device_capability(device)}')\n",
    "\n",
    "log_date = datetime.now().strftime('%m%d-%H%M')\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(asctime)s: %(message)s',\n",
    "                    datefmt='%H:%M:%S',\n",
    "                    filename='/content/GPT2-345M-' + log_date + '.txt',\n",
    "                    filemode='w')\n",
    "\n",
    "logger1 = logging.getLogger('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N1EMBkrHxXb_"
   },
   "source": [
    "#### Convert model to pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22273,
     "status": "ok",
     "timestamp": 1561480004517,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "9pvNNUnrG0MG",
    "outputId": "50af0de6-7c4e-4c91-8eb8-beb8588799bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting TensorFlow checkpoint from /content/models/345M\n",
      "Loading TF weight model/h0/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h0/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h0/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h0/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h0/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h0/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h0/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h0/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h0/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h0/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h0/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h0/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h1/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h1/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h1/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h1/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h1/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h1/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h1/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h1/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h1/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h1/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h1/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h1/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h10/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h10/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h10/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h10/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h10/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h10/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h10/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h10/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h10/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h10/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h10/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h10/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h11/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h11/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h11/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h11/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h11/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h11/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h11/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h11/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h11/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h11/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h11/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h11/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h12/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h12/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h12/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h12/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h12/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h12/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h12/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h12/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h12/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h12/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h12/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h12/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h13/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h13/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h13/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h13/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h13/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h13/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h13/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h13/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h13/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h13/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h13/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h13/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h14/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h14/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h14/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h14/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h14/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h14/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h14/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h14/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h14/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h14/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h14/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h14/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h15/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h15/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h15/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h15/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h15/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h15/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h15/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h15/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h15/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h15/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h15/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h15/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h16/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h16/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h16/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h16/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h16/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h16/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h16/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h16/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h16/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h16/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h16/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h16/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h17/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h17/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h17/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h17/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h17/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h17/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h17/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h17/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h17/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h17/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h17/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h17/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h18/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h18/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h18/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h18/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h18/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h18/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h18/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h18/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h18/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h18/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h18/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h18/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h19/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h19/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h19/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h19/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h19/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h19/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h19/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h19/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h19/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h19/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h19/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h19/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h2/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h2/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h2/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h2/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h2/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h2/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h2/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h2/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h2/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h2/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h2/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h2/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h20/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h20/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h20/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h20/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h20/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h20/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h20/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h20/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h20/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h20/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h20/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h20/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h21/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h21/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h21/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h21/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h21/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h21/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h21/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h21/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h21/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h21/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h21/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h21/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h22/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h22/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h22/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h22/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h22/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h22/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h22/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h22/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h22/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h22/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h22/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h22/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h23/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h23/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h23/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h23/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h23/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h23/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h23/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h23/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h23/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h23/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h23/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h23/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h3/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h3/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h3/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h3/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h3/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h3/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h3/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h3/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h3/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h3/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h3/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h3/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h4/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h4/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h4/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h4/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h4/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h4/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h4/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h4/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h4/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h4/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h4/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h4/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h5/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h5/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h5/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h5/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h5/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h5/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h5/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h5/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h5/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h5/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h5/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h5/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h6/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h6/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h6/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h6/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h6/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h6/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h6/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h6/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h6/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h6/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h6/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h6/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h7/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h7/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h7/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h7/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h7/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h7/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h7/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h7/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h7/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h7/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h7/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h7/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h8/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h8/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h8/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h8/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h8/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h8/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h8/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h8/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h8/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h8/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h8/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h8/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/h9/attn/c_attn/b with shape [3072]\n",
      "Loading TF weight model/h9/attn/c_attn/w with shape [1, 1024, 3072]\n",
      "Loading TF weight model/h9/attn/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h9/attn/c_proj/w with shape [1, 1024, 1024]\n",
      "Loading TF weight model/h9/ln_1/b with shape [1024]\n",
      "Loading TF weight model/h9/ln_1/g with shape [1024]\n",
      "Loading TF weight model/h9/ln_2/b with shape [1024]\n",
      "Loading TF weight model/h9/ln_2/g with shape [1024]\n",
      "Loading TF weight model/h9/mlp/c_fc/b with shape [4096]\n",
      "Loading TF weight model/h9/mlp/c_fc/w with shape [1, 1024, 4096]\n",
      "Loading TF weight model/h9/mlp/c_proj/b with shape [1024]\n",
      "Loading TF weight model/h9/mlp/c_proj/w with shape [1, 4096, 1024]\n",
      "Loading TF weight model/ln_f/b with shape [1024]\n",
      "Loading TF weight model/ln_f/g with shape [1024]\n",
      "Loading TF weight model/wpe with shape [1024, 1024]\n",
      "Loading TF weight model/wte with shape [50257, 1024]\n",
      "Initialize PyTorch weight ['h0', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h0', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h0', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h0', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h0', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h0', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h0', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h0', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h0', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h0', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h0', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h0', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h1', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h1', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h1', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h1', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h1', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h1', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h1', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h1', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h1', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h1', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h1', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h1', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h10', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h10', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h10', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h10', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h10', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h10', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h10', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h10', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h10', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h10', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h10', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h10', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h11', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h11', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h11', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h11', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h11', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h11', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h11', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h11', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h11', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h11', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h11', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h11', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h12', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h12', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h12', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h12', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h12', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h12', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h12', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h12', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h12', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h12', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h12', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h12', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h13', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h13', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h13', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h13', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h13', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h13', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h13', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h13', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h13', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h13', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h13', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h13', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h14', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h14', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h14', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h14', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h14', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h14', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h14', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h14', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h14', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h14', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h14', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h14', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h15', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h15', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h15', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h15', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h15', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h15', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h15', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h15', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h15', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h15', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h15', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h15', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h16', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h16', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h16', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h16', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h16', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h16', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h16', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h16', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h16', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h16', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h16', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h16', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h17', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h17', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h17', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h17', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h17', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h17', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h17', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h17', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h17', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h17', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h17', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h17', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h18', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h18', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h18', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h18', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h18', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h18', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h18', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h18', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h18', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h18', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h18', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h18', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h19', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h19', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h19', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h19', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h19', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h19', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h19', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h19', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h19', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h19', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h19', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h19', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h2', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h2', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h2', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h2', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h2', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h2', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h2', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h2', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h2', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h2', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h2', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h2', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h20', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h20', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h20', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h20', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h20', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h20', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h20', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h20', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h20', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h20', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h20', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h20', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h21', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h21', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h21', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h21', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h21', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h21', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h21', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h21', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h21', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h21', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h21', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h21', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h22', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h22', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h22', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h22', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h22', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h22', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h22', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h22', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h22', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h22', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h22', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h22', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h23', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h23', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h23', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h23', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h23', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h23', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h23', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h23', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h23', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h23', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h23', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h23', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h3', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h3', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h3', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h3', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h3', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h3', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h3', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h3', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h3', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h3', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h3', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h3', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h4', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h4', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h4', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h4', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h4', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h4', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h4', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h4', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h4', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h4', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h4', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h4', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h5', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h5', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h5', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h5', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h5', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h5', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h5', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h5', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h5', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h5', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h5', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h5', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h6', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h6', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h6', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h6', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h6', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h6', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h6', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h6', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h6', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h6', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h6', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h6', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h7', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h7', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h7', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h7', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h7', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h7', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h7', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h7', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h7', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h7', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h7', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h7', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h8', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h8', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h8', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h8', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h8', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h8', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h8', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h8', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h8', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h8', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h8', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h8', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h9', 'attn', 'c_attn', 'b']\n",
      "Initialize PyTorch weight ['h9', 'attn', 'c_attn', 'w']\n",
      "Initialize PyTorch weight ['h9', 'attn', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h9', 'attn', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['h9', 'ln_1', 'b']\n",
      "Initialize PyTorch weight ['h9', 'ln_1', 'g']\n",
      "Initialize PyTorch weight ['h9', 'ln_2', 'b']\n",
      "Initialize PyTorch weight ['h9', 'ln_2', 'g']\n",
      "Initialize PyTorch weight ['h9', 'mlp', 'c_fc', 'b']\n",
      "Initialize PyTorch weight ['h9', 'mlp', 'c_fc', 'w']\n",
      "Initialize PyTorch weight ['h9', 'mlp', 'c_proj', 'b']\n",
      "Initialize PyTorch weight ['h9', 'mlp', 'c_proj', 'w']\n",
      "Initialize PyTorch weight ['ln_f', 'b']\n",
      "Initialize PyTorch weight ['ln_f', 'g']\n",
      "Initialize PyTorch weight ['wpe']\n",
      "Initialize PyTorch weight ['wte']\n",
      "Save PyTorch model to /content/models/345M/pytorch_model.bin\n",
      "Save configuration file to /content/models/345M/config.json\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = '/content/models/345M'\n",
    "\n",
    "convert_gpt2_checkpoint_to_pytorch.convert_gpt2_checkpoint_to_pytorch(\n",
    "    MODEL_PATH,\n",
    "    MODEL_PATH + '/config.json',\n",
    "    MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsbnNsmD240i"
   },
   "source": [
    "#### Load & process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17291,
     "status": "ok",
     "timestamp": 1561481183228,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "WQn4UwKfW_B1",
    "outputId": "92f811f8-4a04-47c0-d1b2-73e437d5796d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train records: 438223 val records: 109556\n"
     ]
    }
   ],
   "source": [
    "def get_inputs(df_in, train_val_split):\n",
    "    # Returns: train_texts, train_labels, val_texts, val_labels\n",
    "    #    ( _texts: np.array of str )\n",
    "    #    ( labels: np.array of np.int64 )\n",
    "\n",
    "    # ...LOADER CODE...\n",
    "    \n",
    "    return train_texts, train_labels, val_texts, val_labels\n",
    "\n",
    "train_texts, train_labels, val_texts, val_labels = get_inputs(train1, 0.8)\n",
    "\n",
    "print(f'train records: {len(train_texts)} val records: {len(val_texts)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGVaeSo_TFhQ"
   },
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "79xVuzs-R_u2"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 128 # text sequences truncated to MAX_LEN-2 to accomodate special tokens\n",
    "BATCH_SIZE = 8\n",
    "N_EPOCHS = 1\n",
    "ETA = 8e-6\n",
    "OPTIMIZER_WARMUP = 0.05\n",
    "GRAD_ACCUM_STEPS = 4 # gradient accumulation: step optimizer every # steps\n",
    "AMP_OPT_LEVEL = 'O1' # https://nvidia.github.io/apex/amp.html#opt-levels\n",
    "LM_COEFF = 0.01  # weighting for language modeling loss vs. classifier loss\n",
    "LOG_INTERVAL = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZE6SyYMpLV6d"
   },
   "source": [
    "#### Encode train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 33577,
     "status": "ok",
     "timestamp": 1561480029627,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "hF_pr6c9Qw8T",
    "outputId": "f30353d0-a54c-4562-cbb9-8d38400439a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13146/13146 [00:06<00:00, 2064.90it/s]\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_TOKENS = ['<BOS>', '<SEP>', '<CLS>']\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_PATH,\n",
    "                                               special_tokens=SPECIAL_TOKENS)\n",
    "\n",
    "special_tokens_ids = list(tokenizer.convert_tokens_to_ids(token) for token in SPECIAL_TOKENS)\n",
    "\n",
    "def encode(sentences, labels, tokenizer, max_len, special_tokens):\n",
    "    assert len(sentences) == len(labels)\n",
    "    n_records = len(sentences)\n",
    "\n",
    "    input_ids = np.zeros((n_records, 1, max_len), dtype=np.int64)\n",
    "    mc_token_ids = np.zeros((n_records, 1), dtype=np.int64)\n",
    "    lm_labels = np.full((n_records, 1, max_len), fill_value=-1, dtype=np.int64)\n",
    "    mc_labels = np.zeros((n_records, 1), dtype=np.int64)\n",
    "\n",
    "    BOS, SEP, CLS = special_tokens\n",
    "\n",
    "    for i, (sentence, label) in enumerate(tqdm(zip(sentences, labels),\n",
    "                                               total=len(sentences),\n",
    "                                               mininterval=10)):\n",
    " \n",
    "        tokens = tokenizer.tokenize(sentence)[:max_len-2]\n",
    "\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids([BOS] + tokens + [CLS])\n",
    "        input_ids[i, 0, :len(indexed_tokens)] = indexed_tokens\n",
    "        mc_token_ids[i, 0] = len(indexed_tokens) - 1\n",
    "        lm_labels[i, 0, :len(indexed_tokens)] = indexed_tokens\n",
    "\n",
    "        mc_labels[i, 0] = label\n",
    "        \n",
    "    all_inputs = (input_ids, mc_token_ids, lm_labels, mc_labels)\n",
    "\n",
    "    return tuple(torch.tensor(t) for t in all_inputs)\n",
    "\n",
    "\n",
    "train_seqs = encode(train_texts,\n",
    "                    train_labels,\n",
    "                    tokenizer,\n",
    "                    MAX_LEN,\n",
    "                    SPECIAL_TOKENS)\n",
    "\n",
    "train_ds = TensorDataset(*train_seqs)\n",
    "train_sampler = RandomSampler(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LvFLcBRs3LHa"
   },
   "source": [
    "#### Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n24FobbPXLs_"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = GPT2DoubleHeadsModel.from_pretrained(MODEL_PATH,\n",
    "                                                  num_special_tokens=len(SPECIAL_TOKENS))\n",
    "model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WC9PCpoMddD"
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 651017,
     "status": "ok",
     "timestamp": 1561480713921,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "M0oKv7HlMeGe",
    "outputId": "4a765093-fd2a-413b-ac7f-de377825f0c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   0%|          | 0/1644 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Training:   5%|▍         | 80/1644 [00:30<09:55,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1644/1644 [10:49<00:00,  2.53it/s, clf_loss=0.545, lm_loss=7.34]\n"
     ]
    }
   ],
   "source": [
    "optimizer = OpenAIAdam(optimizer_grouped_parameters,\n",
    "                       lr=ETA,\n",
    "                       warmup=OPTIMIZER_WARMUP,\n",
    "                       t_total=N_EPOCHS * np.ceil(len(train_texts) / BATCH_SIZE))\n",
    "\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=AMP_OPT_LEVEL, verbosity=1)\n",
    "\n",
    "model = model.train()\n",
    "\n",
    "lm_losses = []\n",
    "clf_losses = []\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "logger1.info(f'train hparams:\\n   train recs: {len(train_texts):,}\\n'\n",
    "             + f'   max_len: {MAX_LEN}\\n   n_epochs: {N_EPOCHS}\\n'\n",
    "             + f'   batch size: {BATCH_SIZE}\\n   eta: {ETA}\\n'\n",
    "             + f'   accumulation steps: {GRAD_ACCUM_STEPS}\\n'\n",
    "             + f'   lm loss coeff: {LM_COEFF}\\n   opt_level: {AMP_OPT_LEVEL}'\n",
    "           )\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loader = DataLoader(train_ds,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=BATCH_SIZE)\n",
    "    n_batches = len(train_loader)\n",
    "    optimizer.zero_grad()\n",
    "    tq = tqdm(train_loader, desc=\"Training\", mininterval=30, maxinterval=60)\n",
    "\n",
    "    for step, batch in enumerate(tq):\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        input_ids, mc_token_ids, lm_labels, mc_labels = batch\n",
    "\n",
    "        lm_loss, clf_loss = model(input_ids, mc_token_ids,\n",
    "                                  lm_labels,\n",
    "                                  mc_labels=mc_labels)\n",
    "                \n",
    "        loss = (LM_COEFF * lm_loss.to(device) + clf_loss.to(device)).to(device)\n",
    "\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "\n",
    "        loss = loss.to(device)\n",
    "\n",
    "        if (step+1) % GRAD_ACCUM_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batch_lm_loss = lm_loss.item()\n",
    "        batch_clf_loss = clf_loss.item()\n",
    "        lm_losses.append(batch_lm_loss)\n",
    "        clf_losses.append(batch_clf_loss)\n",
    "\n",
    "        if (step+1) % LOG_INTERVAL == 0:\n",
    "            clf_loss_mean = sum(clf_losses[(step+1-LOG_INTERVAL):]) / LOG_INTERVAL\n",
    "            lm_loss_mean  = sum(lm_losses[(step+1-LOG_INTERVAL):]) / LOG_INTERVAL\n",
    "            logstr = f'step {step+1} of {n_batches} clf_loss {clf_loss_mean:.4f} lm_loss {lm_loss_mean:.4f}'\n",
    "            logger1.info(logstr)\n",
    "            tq.set_postfix(lm_loss=lm_loss_mean, clf_loss=clf_loss_mean)\n",
    "\n",
    "logger1.info('train complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyYVu2I2PB4i"
   },
   "source": [
    "#### Save trained model\n",
    "(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oiwRR4ZbJSue"
   },
   "outputs": [],
   "source": [
    "SAVED_MODEL_FNAME = 'gpt2_345M_pytorch.bin'\n",
    "SAVED_MODEL_DIR = '/content/saved_models'\n",
    "if not os.path.exists(SAVED_MODEL_DIR):\n",
    "    os.makedirs(SAVED_MODEL_DIR)\n",
    "\n",
    "torch.save(model.state_dict(), os.path.join(SAVED_MODEL_DIR, SAVED_MODEL_FNAME))\n",
    "logger1.info(f'Model saved as {SAVED_MODEL_DIR + \"/\" + SAVED_MODEL_FNAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_qVnDcmRACA"
   },
   "source": [
    "#### Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1561419511723,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "MTIaxNwIPImt",
    "outputId": "ea669d6d-3cba-474a-b53f-c3a68d72e0bb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAADSCAYAAABJsAYRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nov8P5OAXasT4NwAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 3))\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "plt.plot(clf_losses, 'o', color='b', ms=1, alpha=0.1)\n",
    "plt.title('classification loss', fontsize=12, loc='left')\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "plt.plot(lm_losses, 'o', color='b', ms=1, alpha=0.1)\n",
    "plt.title('lm loss', fontsize=12, loc='left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hZZmclZy3kT1"
   },
   "source": [
    "#### Encode eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1778,
     "status": "ok",
     "timestamp": 1561480754692,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "u_bVKpFrO8p_",
    "outputId": "9a9d1834-8894-4241-bac9-cd76fe8f3a49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3287/3287 [00:01<00:00, 2470.10it/s]\n"
     ]
    }
   ],
   "source": [
    "val_seqs = encode(val_texts,\n",
    "                  val_labels,\n",
    "                  tokenizer,\n",
    "                  MAX_LEN,\n",
    "                  SPECIAL_TOKENS)\n",
    "\n",
    "val_ds = TensorDataset(*val_seqs)\n",
    "val_sampler = SequentialSampler(val_ds)\n",
    "\n",
    "val_loader = DataLoader(val_ds,\n",
    "                        sampler=val_sampler,\n",
    "                        batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A769rWgrPTK_"
   },
   "source": [
    "#### Load trained model\n",
    "Skip this step if training and evaluating in the same runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DBuFKv_5PMRN"
   },
   "outputs": [],
   "source": [
    "SAVED_MODEL_FNAME = 'gpt2_345M_pytorch.bin'\n",
    "SAVED_MODEL_DIR = '/content/saved_models'\n",
    "model.load_state_dict(torch.load(os.path.join(SAVED_MODEL_DIR, SAVED_MODEL_FNAME)))\n",
    "logger1.info(f'Model loaded from {SAVED_MODEL_DIR + \"/\" + SAVED_MODEL_FNAME}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iBIWAk4G3hBn"
   },
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 52489,
     "status": "ok",
     "timestamp": 1561480809988,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "KiAqw0gNXOP3",
    "outputId": "0b696398-36f0-4f67-fc46-c3877e50ceac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 411/411 [00:51<00:00,  7.94it/s]\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "val_logits = []\n",
    "val_preds = []\n",
    "\n",
    "tq = tqdm(enumerate(val_loader), total=len(val_loader), mininterval=10)\n",
    "for i, batch in tq:\n",
    "    input_ids, mc_token_ids, _, _ = batch\n",
    "    \n",
    "    _, mc_logits_batch, _ = model(input_ids.to(device),\n",
    "                                  mc_token_ids.to(device))\n",
    "    \n",
    "    preds_batch = torch.sigmoid(mc_logits_batch).to(device)\n",
    "    preds_batch = preds_batch.detach().cpu().squeeze().tolist()\n",
    "    \n",
    "    val_preds.extend(preds_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14713451,
     "status": "ok",
     "timestamp": 1561257784254,
     "user": {
      "displayName": "Michael Hamby",
      "photoUrl": "",
      "userId": "06612384245860910180"
     },
     "user_tz": 420
    },
    "id": "QLptEt8ajz02",
    "outputId": "43c4a906-5e33-4a04-cc6a-7219b342c758"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhoMkqfH/AIj+UenYZvpbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(val_preds, bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R5LJmkuwuRyj"
   },
   "source": [
    "#### Validation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1561480954160,
     "user": {
      "displayName": "michael hamby",
      "photoUrl": "",
      "userId": "11458401093116257828"
     },
     "user_tz": 420
    },
    "id": "889FP6h5RLHm",
    "outputId": "04f9a86f-882c-482e-df68-47a5556cea75"
   },
   "outputs": [],
   "source": [
    "val_preds = np.array(val_preds)\n",
    "val_AUC = metrics.roc_auc_score(val_labels, val_preds)\n",
    "val_acc = metrics.accuracy_score(val_labels.astype('int'), val_preds.astype('int'))\n",
    "logger1.info(f'Eval set AUC = {val_AUC:.4f}')\n",
    "logger1.info(f'Eval set accuracy = {val_acc:.4f}')\n",
    "print(f'AUC {val_AUC:.4f}, acc {val_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GPT2_345M_v2.ipynb",
   "provenance": [
    {
     "file_id": "1RsZjlqL5VFcjjVtOJ_N5BdVSlDYqB85J",
     "timestamp": 1560881687844
    },
    {
     "file_id": "1HarqixjSTvW_C6nDvg2M2umlCVOQXrga",
     "timestamp": 1560867560358
    },
    {
     "file_id": "1RGHO_lRwx46YeT5H14g4x0wiGt5oEwnQ",
     "timestamp": 1560826213011
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
